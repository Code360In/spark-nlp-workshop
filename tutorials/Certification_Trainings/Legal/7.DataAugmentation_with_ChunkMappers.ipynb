{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfbbcfc0-e0b7-4c25-8bd7-c64d90f836d1",
   "metadata": {},
   "source": [
    "# Financial Data Augmentation with Chunk Mappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d818acfe-3b90-4e05-93c9-74e67fc55a13",
   "metadata": {},
   "source": [
    "# Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8626954-14d2-42f9-bdec-e259f66f9d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "with open('../../spark_nlp_for_healthcare_spark_ocr_4.0.2.json') as f:\n",
    "    license_keys = json.load(f)\n",
    "    \n",
    "import os\n",
    "locals().update(license_keys)\n",
    "os.environ.update(license_keys)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79b84900-4c71-4119-92aa-7369ae96aa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparknlp version: 4.0.2\n",
      "sparknlp_jsl version: 4.0.2\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "import sparknlp_jsl\n",
    "\n",
    "print(\"sparknlp version:\",sparknlp.version())\n",
    "print(\"sparknlp_jsl version:\", sparknlp_jsl.version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec74cbb6-b4f8-4eb8-bc55-44a3791bc338",
   "metadata": {},
   "source": [
    "# Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22657a2e-1c7a-4784-afb3-5bbbb7fe3cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP Version : 4.0.2\n",
      "Spark NLP_JSL Version : 4.0.2\n",
      ":: loading settings :: url = jar:file:/home/jovyan/work/persistent/.local/share/jupyter/venvs/spanishdeid/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ef109a49-aa02-45fb-a4cd-6f0a75765576;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;4.0.2 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.2 in central\n",
      ":: resolution report :: resolve 439ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;4.0.2 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.2 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   0   |   0   |   0   ||   17  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ef109a49-aa02-45fb-a4cd-6f0a75765576\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 17 already retrieved (0kB/13ms)\n",
      "22/09/01 10:32:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-juan-40johnsnowlabs-2ecom.jupyter-notebooks.notebookhub.svc.cluster.local:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark NLP Licensed</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f71df18d280>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp_jsl\n",
    "import sparknlp\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print (\"Spark NLP Version :\", sparknlp.version())\n",
    "print (\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
    "\n",
    "spark = sparknlp_jsl.start(license_keys['SECRET'])\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cd4221-fbca-4ca1-86a9-65e6264c4ad1",
   "metadata": {},
   "source": [
    "# About Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9835fd-9def-44e4-b022-e8db0f045fec",
   "metadata": {},
   "source": [
    "__Data Augmentation__ is the process of increase an extracted datapoint with external sources. \n",
    "\n",
    "For example, let's suppose I work with a document which mentions the company _Amazon_. We could be talking about stock prices, or some legal litigations, or just a commercial agreement with a provider, among others.\n",
    "\n",
    "In the document, we can extract `Amazon` using NER as an Organization, but that's all the information available about `Amazon` in that document.\n",
    "\n",
    "Well, with __Data Augmentation__, we can use external sources, as _SEC Edgar, Crunchbase, Nasdaq_ or even _Wikipedia_, to enrich `Amazon` with much more information, allowing us to take better decisions.\n",
    "\n",
    "Let's see how to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef8c0e5-6793-4db5-ab39-f6381c9e500d",
   "metadata": {},
   "source": [
    "# Step 1: Name Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f9fe0-4d7c-4d6e-afc1-47a59b99f529",
   "metadata": {},
   "source": [
    "Let's suppose we get this news from scrapping the Internet, or from Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27f44267-72be-45de-afaa-a2c911195d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"We have entered into a definitive merger agreement with Amazon.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a5e806-9659-4b41-8ab6-38f81b26797f",
   "metadata": {},
   "source": [
    "We use NER to extract the companies name, in this case, Amazon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb765952-24c2-48b6-8d86-5413b13bd9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_detector_dl download started this may take some time.\n",
      "Approximate size to download 514.9 KB\n",
      "[OK!]\n",
      "bert_embeddings_sec_bert_base download started this may take some time.\n",
      "Approximate size to download 390.4 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "        .setInputCol(\"text\")\\\n",
    "        .setOutputCol(\"document\")\n",
    "        \n",
    "sentenceDetector = SentenceDetectorDLModel.pretrained(\"sentence_detector_dl\",\"xx\")\\\n",
    "        .setInputCols([\"document\"])\\\n",
    "        .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "        .setInputCols([\"sentence\"])\\\n",
    "        .setOutputCol(\"token\")\n",
    "\n",
    "embeddings = BertEmbeddings.pretrained(\"bert_embeddings_sec_bert_base\",\"en\") \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "ner_model = LegalNerModel.pretrained(\"legner_orgs_prods_alias\", \"en\", \"legal/models\")\\\n",
    "        .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
    "        .setOutputCol(\"ner\")\n",
    "        \n",
    "ner_converter = NerConverter()\\\n",
    "        .setInputCols([\"sentence\",\"token\",\"ner\"])\\\n",
    "        .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[\n",
    "        documentAssembler,\n",
    "        sentenceDetector,\n",
    "        tokenizer,\n",
    "        embeddings,\n",
    "        ner_model,\n",
    "        ner_converter,\n",
    "])\n",
    "\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "\n",
    "model = nlpPipeline.fit(empty_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eae9a4-52e1-400e-a1dd-effc6ed1da35",
   "metadata": {},
   "source": [
    "## We use LightPipelines to get the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76a05a20-b9b1-4868-a198-0e950c05a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_ner = LightPipeline(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19545c7d-1738-47be-afdd-5f44236a4a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': ['We have entered into a definitive merger agreement with Amazon.'],\n",
       " 'ner_chunk': ['Amazon'],\n",
       " 'token': ['We',\n",
       "  'have',\n",
       "  'entered',\n",
       "  'into',\n",
       "  'a',\n",
       "  'definitive',\n",
       "  'merger',\n",
       "  'agreement',\n",
       "  'with',\n",
       "  'Amazon',\n",
       "  '.'],\n",
       " 'ner': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O'],\n",
       " 'embeddings': ['We',\n",
       "  'have',\n",
       "  'entered',\n",
       "  'into',\n",
       "  'a',\n",
       "  'definitive',\n",
       "  'merger',\n",
       "  'agreement',\n",
       "  'with',\n",
       "  'Amazon',\n",
       "  '.'],\n",
       " 'sentence': ['We have entered into a definitive merger agreement with Amazon.']}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_result = lp_ner.annotate(text)\n",
    "ner_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe41161-c8fd-467e-9fff-5d4fe1cb5160",
   "metadata": {},
   "source": [
    "Alright! Amazon has been detected as an organization. \n",
    "\n",
    "Now, let's augment `Amazon` with more information about the company, given that there are no more details in the tweet I can use.\n",
    "\n",
    "But before __augmenting__, there is a very important step we need to carry out: `Company Name Normalization`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e2808-3550-46d9-835b-f747cac4123c",
   "metadata": {},
   "source": [
    "# Step 2: Company Names Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e55a84c-2806-4a0e-b30f-90a43dc497ca",
   "metadata": {},
   "source": [
    "Let's suppose we want to manually get information about Amazon.\n",
    "\n",
    "Since it's a public US company, we can go to [SEC Edgar's database](https://www.sec.gov/edgar/searchedgar/companysearch) and look for it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f414e7-d727-4b8a-ba8a-8b0c644bb7da",
   "metadata": {},
   "source": [
    "Unfortunately, `Amazon` is not the official name of the company, which means no entry for `Amazon` is available. That's were __Company Names Normalization__ comes in handy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb8eb7f-1bda-454c-8318-bb4df34f0b6a",
   "metadata": {},
   "source": [
    "`Company Name Normalization` is the process of obtaining the name of the company used by data providers, usually the \"official\" name of the company.\n",
    "\n",
    "Sometimes, some data providers may have different versions of the name with different punctuation. For example, for Meta:\n",
    "- Meta Platforms, Inc.\n",
    "- Meta Platforms Inc.\n",
    "- Meta Platforms, Inc\n",
    "- etc\n",
    "\n",
    "So, it's mandatory we do `Company Normalization` taking into account the database / datasource provider we want to extract data from. The data providers we have are:\n",
    "- SEC Edgar\n",
    "- Crunchbase until 2015\n",
    "- Wikidata (in progress)\n",
    "\n",
    "Let's normalize `Amazon` to the official name in _SEC Edgar_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2564fd32-99ec-451c-ae34-2792cf3036ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "embeddings = UniversalSentenceEncoder.pretrained(\"tfhub_use\", \"en\") \\\n",
    "      .setInputCols(\"document\") \\\n",
    "      .setOutputCol(\"sentence_embeddings\")\n",
    "    \n",
    "resolver = SentenceEntityResolverModel.pretrained(\"legel_edgar_company_name\", \"en\", \"legal/models\")\\\n",
    "      .setInputCols([\"text\", \"sentence_embeddings\"]) \\\n",
    "      .setOutputCol(\"resolution\")\\\n",
    "      .setDistanceFunction(\"EUCLIDEAN\")\n",
    "\n",
    "pipelineModel = PipelineModel(\n",
    "      stages = [\n",
    "          documentAssembler,\n",
    "          embeddings,\n",
    "          resolver])\n",
    "\n",
    "lp_res = LightPipeline(pipelineModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36a6d5f7-6477-4219-acf7-53a95d1ebea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amazon']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_result['ner_chunk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c319f7b8-fe7e-4408-9960-15e7675a36c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'document': ['Amazon'],\n",
       "  'sentence_embeddings': ['Amazon'],\n",
       "  'resolution': ['AMAZON COM INC']}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "el_res = lp_res.annotate(ner_result['ner_chunk'])\n",
    "el_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109efb72-bfae-413b-b1cb-ef1c57b9b66d",
   "metadata": {},
   "source": [
    "Here is our normalized name for Amazon: `AMAZON COM INC`.\n",
    "\n",
    "Now, let's see which information is available in Edgar database for `AMAZON COM INC`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b8b1d-2754-4338-acc4-d74aeab8a673",
   "metadata": {},
   "source": [
    "# Steps 1 and 2 in the same pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46ed95a6-aa6e-47b5-897e-6430249f9532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_detector_dl download started this may take some time.\n",
      "Approximate size to download 514.9 KB\n",
      "[OK!]\n",
      "bert_embeddings_sec_bert_base download started this may take some time.\n",
      "Approximate size to download 390.4 MB\n",
      "[OK!]\n",
      "tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "        .setInputCol(\"text\")\\\n",
    "        .setOutputCol(\"document\")\n",
    "        \n",
    "sentenceDetector = SentenceDetectorDLModel.pretrained(\"sentence_detector_dl\",\"xx\")\\\n",
    "        .setInputCols([\"document\"])\\\n",
    "        .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "        .setInputCols([\"sentence\"])\\\n",
    "        .setOutputCol(\"token\")\n",
    "\n",
    "embeddings = BertEmbeddings.pretrained(\"bert_embeddings_sec_bert_base\",\"en\") \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "ner_model = FinanceNerModel.pretrained(\"legner_orgs_prods_alias\", \"en\", \"legal/models\")\\\n",
    "        .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
    "        .setOutputCol(\"ner\")\n",
    "        \n",
    "ner_converter = NerConverter()\\\n",
    "        .setInputCols([\"sentence\",\"token\",\"ner\"])\\\n",
    "        .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "chunk2doc = Chunk2Doc()\\\n",
    "        .setInputCols(\"ner_chunk\")\\\n",
    "        .setOutputCol(\"ner_chunk_doc\")\n",
    "\n",
    "sentence_embeddings = UniversalSentenceEncoder.pretrained(\"tfhub_use\", \"en\") \\\n",
    "      .setInputCols(\"ner_chunk_doc\") \\\n",
    "      .setOutputCol(\"sentence_embeddings\")\n",
    "    \n",
    "resolver = SentenceEntityResolverModel.pretrained(\"legel_edgar_company_name\", \"en\", \"legal/models\")\\\n",
    "      .setInputCols([\"text\", \"sentence_embeddings\"]) \\\n",
    "      .setOutputCol(\"resolution\")\\\n",
    "      .setDistanceFunction(\"EUCLIDEAN\")\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[\n",
    "        documentAssembler,\n",
    "        sentenceDetector,\n",
    "        tokenizer,\n",
    "        embeddings,\n",
    "        ner_model,\n",
    "        ner_converter,\n",
    "        chunk2doc,\n",
    "        sentence_embeddings,\n",
    "        resolver\n",
    "])\n",
    "\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "\n",
    "model = nlpPipeline.fit(empty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9647e98c-6a53-4892-93ec-c1187110b024",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_model = LightPipeline(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b29c1146-c38c-4def-8c38-30f494461d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': ['We have entered into a definitive merger agreement with Amazon.'],\n",
       " 'ner_chunk': ['Amazon'],\n",
       " 'sentence_embeddings': ['Amazon'],\n",
       " 'resolution': ['AMAZON COM INC'],\n",
       " 'token': ['We',\n",
       "  'have',\n",
       "  'entered',\n",
       "  'into',\n",
       "  'a',\n",
       "  'definitive',\n",
       "  'merger',\n",
       "  'agreement',\n",
       "  'with',\n",
       "  'Amazon',\n",
       "  '.'],\n",
       " 'ner': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O'],\n",
       " 'embeddings': ['We',\n",
       "  'have',\n",
       "  'entered',\n",
       "  'into',\n",
       "  'a',\n",
       "  'definitive',\n",
       "  'merger',\n",
       "  'agreement',\n",
       "  'with',\n",
       "  'Amazon',\n",
       "  '.'],\n",
       " 'ner_chunk_doc': ['Amazon'],\n",
       " 'sentence': ['We have entered into a definitive merger agreement with Amazon.']}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "el_res = lp_model.annotate(text)\n",
    "el_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e51e16-56a2-4dad-a27c-1047a36ecea3",
   "metadata": {},
   "source": [
    "# Step 3: Data Augmentation with Chunk Mappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dff3ed-08f0-4961-ba54-bbffb2606a81",
   "metadata": {},
   "source": [
    "The component which carries out __Data Augmentation__ is called `ChunkMapper`.\n",
    "\n",
    "It's name comes from the way it works: it uses a _Ner Chunk_ to map it to an external data source.\n",
    "\n",
    "As a result, you will get a JSON with a dictionary of additional fields and their values. \n",
    "\n",
    "Let's take a look at how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b246357e-0ab7-489b-9dc0-6d74d3eb97ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "chunkAssembler = Doc2Chunk() \\\n",
    "    .setInputCols(\"document\") \\\n",
    "    .setOutputCol(\"chunk\") \\\n",
    "    .setIsArray(False)\n",
    "\n",
    "CM = ChunkMapperModel().pretrained(\"legmapper_edgar_companyname\", \"en\", \"legal/models\")\\\n",
    "      .setInputCols([\"chunk\"])\\\n",
    "      .setOutputCol(\"mappings\")\n",
    "\n",
    "cm_pipeline = Pipeline(stages=[documentAssembler, chunkAssembler, CM])\n",
    "fit_cm_pipeline = cm_pipeline.fit(empty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "340f193f-dabd-4e41-96e0-0afe0a22ed8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|          text|\n",
      "+--------------+\n",
      "|AMAZON COM INC|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# LightPipelines don't support Doc2Chunk, so we will use here usual transform\n",
    "\n",
    "df = spark.createDataFrame([el_res['resolution']]).toDF(\"text\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b6729d6-32a9-4cea-b88f-0b6b7bf04d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+\n",
      "|          text|            document|               chunk|            mappings|\n",
      "+--------------+--------------------+--------------------+--------------------+\n",
      "|AMAZON COM INC|[{document, 0, 13...|[{chunk, 0, 13, A...|[{labeled_depende...|\n",
      "+--------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = fit_cm_pipeline.transform(df)\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2d4a37d-b0a1-413a-9e36-5489396a042d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(text='AMAZON COM INC', document=[Row(annotatorType='document', begin=0, end=13, result='AMAZON COM INC', metadata={'sentence': '0'}, embeddings=[])], chunk=[Row(annotatorType='chunk', begin=0, end=13, result='AMAZON COM INC', metadata={'sentence': '0', 'chunk': '0'}, embeddings=[])], mappings=[Row(annotatorType='labeled_dependency', begin=0, end=13, result='AMAZON COM INC', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'name', 'all_relations': ''}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='RETAIL-CATALOG & MAIL-ORDER HOUSES [5961]', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'sic', 'all_relations': '[5961'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='5961', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'sic_code', 'all_relations': '0'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='911646860', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'irs_number', 'all_relations': '0:::261631624'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='1231', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'fiscal_year_end', 'all_relations': '0'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='WA', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'state_location', 'all_relations': ''}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='DE', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'state_incorporation', 'all_relations': ''}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='410 TERRY AVENUE NORTH', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'business_street', 'all_relations': '1200 12TH AVENUE S SUITE 1200'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='SEATTLE', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'business_city', 'all_relations': ''}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='WA', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'business_state', 'all_relations': ''}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='98109', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'business_zip', 'all_relations': '98144'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='2062661000', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'business_phone', 'all_relations': ''}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='ABX Holdings, Inc.', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'former_name', 'all_relations': 'ABX AIR INC'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='20080102', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'former_name_date', 'all_relations': '19950728'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='2017-02-10', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'date', 'all_relations': '2016-01-29:::2016-02-10:::2016-09-08:::2016-10-28:::2019-02-01:::2019-10-25:::2018-02-02:::2018-07-27:::2015-01-30:::2015-07-24:::2015-10-23:::2015-12-04:::2014-01-31:::2014-01-30:::2014-02-11:::2014-02-18:::2014-02-19:::2014-02-13:::2014-02-20:::2014-03-06:::2014-04-09:::2014-04-04:::2013-01-30:::2012-02-01:::2011-01-28:::2011-07-27:::2011-10-26:::2022-02-04:::2022-07-29:::2021-02-03:::2021-04-08:::2020-01-31:::2020-05-01:::2020-10-30:::2010-01-29:::2010-07-23:::2010-10-22:::2009-01-30:::2009-07-24:::2009-10-23:::2008-02-11:::2008-07-25:::2007-02-16:::2007-04-26:::2007-07-26:::2007-10-25:::2006-02-17:::2006-07-27:::2006-10-26:::2005-03-11:::2005-07-28:::2004-02-25:::2004-04-23:::2003-02-19:::2003-07-24:::2003-09-24:::2002-01-24:::2003-10-24:::2002-07-26:::2002-10-25:::2000-08-02:::2000-10-30'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='1018724', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'company_id', 'all_relations': ''}, embeddings=[])])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = res.collect()\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd37fe7a-6823-4ab3-a41e-e1e711fdbbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dict = dict()\n",
    "for n in r[0]['mappings']:\n",
    "    json_dict[n.metadata['relation']] = str(n.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35e22829-8c0d-4853-a896-cc9502a567b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"business_city\": \"SEATTLE\",\n",
      "    \"business_phone\": \"2062661000\",\n",
      "    \"business_state\": \"WA\",\n",
      "    \"business_street\": \"410 TERRY AVENUE NORTH\",\n",
      "    \"business_zip\": \"98109\",\n",
      "    \"company_id\": \"1018724\",\n",
      "    \"date\": \"2017-02-10\",\n",
      "    \"fiscal_year_end\": \"1231\",\n",
      "    \"former_name\": \"ABX Holdings, Inc.\",\n",
      "    \"former_name_date\": \"20080102\",\n",
      "    \"irs_number\": \"911646860\",\n",
      "    \"name\": \"AMAZON COM INC\",\n",
      "    \"sic\": \"RETAIL-CATALOG & MAIL-ORDER HOUSES [5961]\",\n",
      "    \"sic_code\": \"5961\",\n",
      "    \"state_incorporation\": \"DE\",\n",
      "    \"state_location\": \"WA\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(json_dict, indent=4, sort_keys=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spanish DEID",
   "language": "python",
   "name": "spanishdeid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
