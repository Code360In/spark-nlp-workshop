{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c168511e-1278-4fe1-9e10-f55f7cb69213"
        },
        "id": "qZyBOOIzgMFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Serving Spark NLP with API. Synapse ML and Fast API with LightPipelines"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "64263998-73cc-4de7-914b-63f09d5f6ce2"
        },
        "id": "P41lOks_gMFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SynapseML"
      ],
      "metadata": {
        "id": "fhoeHWW1lcHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "tags": [],
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6a45dc2c-211e-46c7-8e7d-88aeddf1e6e5"
        },
        "id": "XWoKybSqgMFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import json\n",
        "import os\n",
        "with open('spark_nlp_for_healthcare-3.4.0.json') as f:\n",
        "    license_keys = json.load(f)\n",
        "    \n",
        "locals().update(license_keys)\n",
        "os.environ.update(license_keys)"
      ],
      "metadata": {
        "id": "knjt2yEliM8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.2.0 spark-nlp==$PUBLIC_VERSION"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "eb9af348-1a22-4db6-97be-48d9aca0906c"
        },
        "id": "RGgp_LnugMFk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spark-nlp-jsl==$JSL_VERSION --extra-index-url https://pypi.johnsnowlabs.com/$SECRET"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "59fa8b64-322a-4bb2-94ea-4a5432c0f6bc"
        },
        "id": "LV80dAx3gMFm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "fab81cae-cd66-4b52-8cf3-6cf2f8862357"
        },
        "id": "70Lz2EAugMFm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sparknlpjsl_jar = \"./spark-nlp-jsl.jar\" # Download the FAT jar to include it in the SparkSession. This is reuquired by SynapseML"
      ],
      "metadata": {
        "id": "d3zcmK-8imAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Imports and Spark Session"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e46207ef-867c-48ec-9f1c-219a8e5ff750"
        },
        "id": "AFde6nnSgMFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pyspark\n",
        "import sparknlp\n",
        "import sparknlp_jsl\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp_jsl.annotator import *\n",
        "from sparknlp.training import *\n",
        "from sparknlp.training import CoNLL\n",
        "import time\n",
        "import requests\n",
        "import uuid\n",
        "import json\n",
        "import requests\n",
        "from concurrent.futures import ThreadPoolExecutor\n"
      ],
      "metadata": {
        "scrolled": true,
        "tags": [],
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "71837fea-adb4-4e5f-90d7-0eb474d5067d"
        },
        "id": "HoDZoYJxgMFn",
        "outputId": "c9f0e123-8beb-410d-fbbb-2176ca88ac88"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "<div class=\"ansiout\"></div>",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "html",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
            ]
          },
          "transient": null
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"16G\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
        "    .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:0.9.5,com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.0\")\\\n",
        "    .config(\"spark.jars\", sparknlpjsl_jar)\\\n",
        "    .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\")\\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "b0LKm7ruip_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sparknlp.version())\n",
        "print(sparknlp_jsl.version())"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "0060d257-0ca2-4e43-a641-675db36ba689"
        },
        "id": "7HZXmvaHgMFo",
        "outputId": "614bbae7-ff5c-46b6-c540-060e8281ead0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "<div class=\"ansiout\">3.4.0\n3.4.0\n</div>",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "html",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">3.4.0\n3.4.0\n</div>"
            ]
          },
          "transient": null
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "aff6c8fc-92cc-4166-ae9b-050574fa51a0"
        },
        "id": "gns8bOFJgMFp",
        "outputId": "9f65dc73-20a4-4b21-f5cd-9acb0f00a6ee"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "<div class=\"ansiout\">Out[8]: </div>",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "html",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[8]: </div>"
            ]
          },
          "transient": null
        },
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=7956323724731612#setting/sparkui/0127-082007-5g2tpazx/driver-1729089974107808210\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://10.139.64.12:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
              "textData": null,
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "htmlSandbox",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=7956323724731612#setting/sparkui/0127-082007-5g2tpazx/driver-1729089974107808210\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://10.139.64.12:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
            ]
          },
          "transient": null
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import synapse.ml\n",
        "from synapse.ml.io import *"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7dc0cb8e-01d1-4dae-a266-d2bda3292185"
        },
        "id": "8_f_PasWgMFp",
        "outputId": "7ad6e2a5-309a-473d-94c7-79df761df3aa"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "<div class=\"ansiout\"></div>",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "html",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
            ]
          },
          "transient": null
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing a pipeline with Entity Resolution"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "af6149fb-1df7-476d-b74b-0256789151b0"
        },
        "id": "jzvMA3IJgMFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Annotator that transforms a text column from dataframe into an Annotation ready for NLP\n",
        "document_assembler = DocumentAssembler()\\\n",
        "      .setInputCol(\"text\")\\\n",
        "      .setOutputCol(\"document\")\n",
        "\n",
        "# Sentence Detector DL annotator, processes various sentences per line\n",
        "sentenceDetectorDL = SentenceDetectorDLModel.pretrained(\"sentence_detector_dl_healthcare\", \"en\", 'clinical/models') \\\n",
        "      .setInputCols([\"document\"]) \\\n",
        "      .setOutputCol(\"sentence\")\n",
        "\n",
        "# Tokenizer splits words in a relevant format for NLP\n",
        "tokenizer = Tokenizer()\\\n",
        "      .setInputCols([\"sentence\"])\\\n",
        "      .setOutputCol(\"token\")\n",
        "\n",
        "# WordEmbeddingsModel pretrained \"embeddings_clinical\" includes a model of 1.7Gb that needs to be downloaded\n",
        "word_embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
        "  .setInputCols([\"sentence\", \"token\"])\\\n",
        "  .setOutputCol(\"word_embeddings\")\n",
        "\n",
        "# Named Entity Recognition for clinical concepts.\n",
        "clinical_ner = MedicalNerModel.pretrained(\"ner_clinical\", \"en\", \"clinical/models\") \\\n",
        "      .setInputCols([\"sentence\", \"token\", \"word_embeddings\"]) \\\n",
        "      .setOutputCol(\"ner\")\n",
        "\n",
        "ner_converter_icd = NerConverterInternal() \\\n",
        "      .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
        "      .setOutputCol(\"ner_chunk\")\\\n",
        "      .setWhiteList(['PROBLEM'])\\\n",
        "      .setPreservePosition(False)\n",
        "\n",
        "c2doc = Chunk2Doc()\\\n",
        "      .setInputCols(\"ner_chunk\")\\\n",
        "      .setOutputCol(\"ner_chunk_doc\") \n",
        "\n",
        "sbert_embedder = BertSentenceEmbeddings.pretrained('sbiobert_base_cased_mli', 'en','clinical/models')\\\n",
        "      .setInputCols([\"ner_chunk_doc\"])\\\n",
        "      .setOutputCol(\"sentence_embeddings\")\\\n",
        "      .setCaseSensitive(False)\n",
        "    \n",
        "icd_resolver = SentenceEntityResolverModel.pretrained(\"sbiobertresolve_icd10cm_augmented_billable_hcc\",\"en\", \"clinical/models\") \\\n",
        "     .setInputCols([\"ner_chunk\", \"sentence_embeddings\"]) \\\n",
        "     .setOutputCol(\"icd10cm_code\")\\\n",
        "     .setDistanceFunction(\"EUCLIDEAN\")\n",
        "    \n",
        "\n",
        "# Build up the pipeline\n",
        "resolver_pipeline = Pipeline(\n",
        "    stages = [\n",
        "        document_assembler,\n",
        "        sentenceDetectorDL,\n",
        "        tokenizer,\n",
        "        word_embeddings,\n",
        "        clinical_ner,\n",
        "        ner_converter_icd,\n",
        "        c2doc,\n",
        "        sbert_embedder,\n",
        "        icd_resolver\n",
        "  ])\n",
        "\n",
        "\n",
        "empty_data = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "resolver_p_model = resolver_pipeline.fit(empty_data)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "68f6b971-2097-46f7-a8e3-331c6373b91d"
        },
        "id": "dFxLKwBGgMFq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding a clinical note as a text example"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c642dcd8-8dd1-45e9-9468-fd8a1e524ba0"
        },
        "id": "D7EocdfogMFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clinical_note = \"\"\"A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years \n",
        "    prior to presentation and subsequent type two diabetes mellitus (T2DM), one prior \n",
        "    episode of HTG-induced pancreatitis three years prior to presentation, associated \n",
        "    with an acute hepatitis, and obesity with a body mass index (BMI) of 33.5 kg/m2, \n",
        "    presented with a one-week history of polyuria, polydipsia, poor appetite, and vomiting. \n",
        "    Two weeks prior to presentation, she was treated with a five-day course of amoxicillin \n",
        "    for a respiratory tract infection. She was on metformin, glipizide, and dapagliflozin \n",
        "    for T2DM and atorvastatin and gemfibrozil for HTG. She had been on dapagliflozin for six months \n",
        "    at the time of presentation. Physical examination on presentation was significant for dry oral mucosa; \n",
        "    significantly, her abdominal examination was benign with no tenderness, guarding, or rigidity. Pertinent \n",
        "    laboratory findings on admission were: serum glucose 111 mg/dl, bicarbonate 18 mmol/l, anion gap 20, \n",
        "    creatinine 0.4 mg/dL, triglycerides 508 mg/dL, total cholesterol 122 mg/dL, glycated hemoglobin (HbA1c) \n",
        "    10%, and venous pH 7.27. Serum lipase was normal at 43 U/L. Serum acetone levels could not be assessed \n",
        "    as blood samples kept hemolyzing due to significant lipemia. The patient was initially admitted for \n",
        "    starvation ketosis, as she reported poor oral intake for three days prior to admission. However, \n",
        "    serum chemistry obtained six hours after presentation revealed her glucose was 186 mg/dL, the anion gap \n",
        "    was still elevated at 21, serum bicarbonate was 16 mmol/L, triglyceride level peaked at 2050 mg/dL, and \n",
        "    lipase was 52 U/L. The β-hydroxybutyrate level was obtained and found to be elevated at 5.29 mmol/L - \n",
        "    the original sample was centrifuged and the chylomicron layer removed prior to analysis due to \n",
        "    interference from turbidity caused by lipemia again. The patient was treated with an insulin drip \n",
        "    for euDKA and HTG with a reduction in the anion gap to 13 and triglycerides to 1400 mg/dL, within \n",
        "    24 hours. Her euDKA was thought to be precipitated by her respiratory tract infection in the setting \n",
        "    of SGLT2 inhibitor use. The patient was seen by the endocrinology service and she was discharged on \n",
        "    40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg \n",
        "    two times a day. It was determined that all SGLT2 inhibitors should be discontinued indefinitely. She \n",
        "    had close follow-up with endocrinology post discharge.\"\"\"\n",
        "\n",
        "\n",
        "data = spark.createDataFrame([[clinical_note]]).toDF(\"text\")"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d06acccf-1773-4d4c-b02c-3aa7045c0548"
        },
        "id": "7mOcVmZ6gMFr",
        "outputId": "1827385a-f98c-4557-9675-f9127ccf97b2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "<div class=\"ansiout\"></div>",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "html",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
            ]
          },
          "transient": null
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a JSON file with the clinical note\n",
        "Since SynapseML runs a webservice that accepts HTTP calls with json format"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "82044e07-6c3b-497c-a65a-bbf8d5ba2a12"
        },
        "id": "CXO1JX2jgMFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_json = {\"text\": clinical_note }"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e0e402f4-859d-481e-8cc3-9f5b366bcf73"
        },
        "id": "yac_FVrpgMFs",
        "outputId": "463a65c5-c6c0-4dfd-f636-2dfdb891563d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "<div class=\"ansiout\"></div>",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "html",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
            ]
          },
          "transient": null
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running a Synapse server"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "214d7d56-45af-4669-af9c-c1c0a08fd651"
        },
        "id": "0gjx1_1qgMFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "serving_input = spark.readStream.server() \\\n",
        "    .address(\"localhost\", 9999, \"benchmark_api\") \\\n",
        "    .option(\"name\", \"benchmark_api\") \\\n",
        "    .load() \\\n",
        "    .parseRequest(\"benchmark_api\", data.schema)\n",
        "\n",
        "serving_output = resolver_p_model.transform(serving_input) \\\n",
        "    .makeReply(\"icd10cm_code\")\n",
        "\n",
        "server = serving_output.writeStream \\\n",
        "      .server() \\\n",
        "      .replyTo(\"benchmark_api\") \\\n",
        "      .queryName(\"benchmark_query\") \\\n",
        "      .option(\"checkpointLocation\", \"file:///tmp/checkpoints-{}\".format(uuid.uuid1())) \\\n",
        "      .start()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "87715459-0d21-4bfd-a85a-d9fc3e33d5d8"
        },
        "id": "tWZzC9AzgMFt",
        "outputId": "39a4788a-0af3-4212-a54d-43975c836b34"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "<div class=\"ansiout\">/databricks/spark/python/pyspark/sql/context.py:134: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n</div>",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "html",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/sql/context.py:134: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n</div>"
            ]
          },
          "transient": null
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def post_url(args):\n",
        "    print(f\"- Request {str(args[2])} launched!\")\n",
        "    res = requests.post(args[0], data=args[1])    \n",
        "    print(f\"**Response {str(args[2])} received**\")\n",
        "    return res\n",
        "\n",
        "# If you want to send parallel calls, just add more tuples to list_of_urls array\n",
        "# tuple: (URL from above, json, number_of_call)\n",
        "list_of_urls = [(\"http://localhost:9999/benchmark_api\",json.dumps(data_json), 0)]\n",
        "\n",
        "with ThreadPoolExecutor() as pool:\n",
        "    response_list = list(pool.map(post_url,list_of_urls))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "92daabe4-d70e-433b-8e89-e4b4b4b9c4d8"
        },
        "id": "sGuWqekLgMFt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking Results"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "98482a54-b02a-451a-9d52-fdaa440efdce"
        },
        "id": "x4dXBTVzgMFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (0, len(response_list.json())):\n",
        "  print(response_list.json()[i]['result'])"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "5d7091ed-3e4e-43dd-8729-d7775d3645bd"
        },
        "id": "Y0iHycFZgMFu",
        "outputId": "f527a956-45a0-4f1d-e1d6-1d73c7c27f98"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "<div class=\"ansiout\">O2441\nO2411\nP702\nK8520\nB159\nE669\nZ6841\nR35\nR631\nR630\nR111\nJ988\nP702\nG600\nK130\nR529\nM6283\nR4689\nO046\nE785\nE872\nE639\nH5330\nR799\nR829\nE785\nA832\nG600\nJ988\nO2441\nO2411\nP702\nK8520\nB159\nE669\nZ6841\nR35\nR631\nR630\nR111\nJ988\nP702\nG600\nK130\nR529\nM6283\nR4689\nO046\nE785\nE872\nE639\nH5330\nR799\nR829\nE785\nA832\nG600\nJ988\nO2441\nO2411\nP702\nK8520\nB159\nE669\nZ6841\nR35\nR631\nR630\nR111\nJ988\nP702\nG600\nK130\nR529\nM6283\nR4689\nO046\nE785\nE872\nE639\nH5330\nR799\nR829\nE785\nA832\nG600\nJ988\n</div>",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "html",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">O2441\nO2411\nP702\nK8520\nB159\nE669\nZ6841\nR35\nR631\nR630\nR111\nJ988\nP702\nG600\nK130\nR529\nM6283\nR4689\nO046\nE785\nE872\nE639\nH5330\nR799\nR829\nE785\nA832\nG600\nJ988\nO2441\nO2411\nP702\nK8520\nB159\nE669\nZ6841\nR35\nR631\nR630\nR111\nJ988\nP702\nG600\nK130\nR529\nM6283\nR4689\nO046\nE785\nE872\nE639\nH5330\nR799\nR829\nE785\nA832\nG600\nJ988\nO2441\nO2411\nP702\nK8520\nB159\nE669\nZ6841\nR35\nR631\nR630\nR111\nJ988\nP702\nG600\nK130\nR529\nM6283\nR4689\nO046\nE785\nE872\nE639\nH5330\nR799\nR829\nE785\nA832\nG600\nJ988\n</div>"
            ]
          },
          "transient": null
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Fast API and LightPipeline\n",
        "You can serve SparkNLP + FastAPI on Docker. To do that, we will create a project with the following files:\n",
        "- `Dockerfile`\n",
        "- `requirements.txt`\n",
        "- `entrypoint.sh`\n",
        "- `content/ folder`\n",
        "- `content/main.py`\n",
        "- `content/sparknlp_keys.json`\n",
        "\n",
        "And finally, a `serve.py` file to consume the API, that can be outside your project, in a notebook, etc."
      ],
      "metadata": {
        "id": "LXWB2qBefoUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dockerfile\n",
        "`Dockerfile`"
      ],
      "metadata": {
        "id": "vVEWnuZzhFc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "FROM ubuntu:18.04\n",
        "RUN apt-get update && apt-get -y update\n",
        "\n",
        "RUN apt-get -y update \\\n",
        "    && apt-get install -y wget \\\n",
        "    && apt-get install -y jq \\\n",
        "    && apt-get install -y lsb-release \\\n",
        "    && apt-get install -y openjdk-8-jdk-headless \\\n",
        "    && apt-get install -y build-essential python3-pip \\\n",
        "    && pip3 -q install pip --upgrade \\\n",
        "    && apt-get clean && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* \\\n",
        "         /usr/share/man /usr/share/doc /usr/share/doc-base\n",
        "\n",
        "ENV PYSPARK_DRIVER_PYTHON=python3\n",
        "ENV PYSPARK_PYTHON=python3\n",
        "\n",
        "ENV LC_ALL=C.UTF-8\n",
        "ENV LANG=C.UTF-8\n",
        "\n",
        "EXPOSE 8515\n",
        "\n",
        "COPY requirements.txt /\n",
        "RUN pip install -r /requirements.txt\n",
        "\n",
        "COPY entrypoint.sh /\n",
        "RUN chmod +x /entrypoint.sh\n",
        "\n",
        "COPY ./content/ /content/\n",
        "WORKDIR content/\n",
        "\n",
        "ENTRYPOINT [\"/entrypoint.sh\"]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wHddmQ1Xfn0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other files of the project"
      ],
      "metadata": {
        "id": "XxHVXamhlvTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`requirements.txt`"
      ],
      "metadata": {
        "id": "-Q9N_OevhCxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"pyspark==3.1.2\n",
        "fastapi==0.70.1\n",
        "uvicorn==0.16\n",
        "wget==3.2\n",
        "pandas\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "oG0J0wQeg-4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`entrypoint.sh`"
      ],
      "metadata": {
        "id": "U9D0oDrQhH5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#!/bin/bash\n",
        "\n",
        "export_json () {\n",
        "    for s in $(echo $values | jq -r 'to_entries|map(\"\\(.key)=\\(.value|tostring)\")|.[]' $1 ); do\n",
        "        export $s\n",
        "    done\n",
        "}\n",
        "\n",
        "export_json \"/content/sparknlp_keys.json\"\n",
        "\n",
        "pip install --upgrade spark-nlp-jsl==$JSL_VERSION --user --extra-index-url https://pypi.johnsnowlabs.com/$SECRET\n",
        "\n",
        "if [ $? != 0 ];\n",
        "then\n",
        "    exit 1\n",
        "fi\n",
        "\n",
        "python3 /content/main.py\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VZ-wZ3lOhIP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example to serve 2 pipelines\n",
        "`ner_profiling_clinical` and `clinical_deidentification`"
      ],
      "metadata": {
        "id": "l0lyotF4lyt8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`content/main.py`"
      ],
      "metadata": {
        "id": "G5u8IP82hgPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "import uvicorn\n",
        "import json\n",
        "import os\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp_jsl.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp_jsl\n",
        "import sparknlp\n",
        "from datetime import datetime\n",
        "import warnings \n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "\n",
        "app = FastAPI()\n",
        "event_list = dict()\n",
        "pipelines = {}\n",
        "\n",
        "@app.get(\"/benchmark/pipeline\")\n",
        "async def get_one_sequential_pipeline_result(modelname, text=''):\n",
        "    # print(list(pipelines.keys()))\n",
        "    if modelname is None or modelname not in pipelines.keys():\n",
        "        return json.dumps({'error': f\"{modelname} not in loaded list: {list(pipelines.keys())}\"})\n",
        "\n",
        "    return pipelines[modelname].annotate(text)\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def startup_event():\n",
        "    if 'pipeline_loaded' in event_list:\n",
        "        return\n",
        "    event_list['start_up']=datetime.now()\n",
        "    print(f'startup has been started at {datetime.now()}...', )\n",
        "    print(list(pipelines.keys()))\n",
        "\n",
        "    print(f'****** spark nlp healthcare version fired up {datetime.now()} ******')\n",
        "    event_list['sparknlp_fired']=datetime.now()\n",
        "\n",
        "    print(\"- App started.\")\n",
        "    with open('/content/sparknlp_keys.json', 'r') as f:\n",
        "        license_keys = json.load(f)\n",
        "\n",
        "    params = {'spark.driver.memory': '4G'}\n",
        "    spark = sparknlp_jsl.start(secret=license_keys['SECRET'], params=params)\n",
        "\n",
        "    # Defining license key-value pairs as local variables\n",
        "    locals().update(license_keys)\n",
        "\n",
        "    # Adding license key-value pairs to environment variables\n",
        "    os.environ.update(license_keys)\n",
        "\n",
        "    print(\"-- Spark NLP Version :\", sparknlp.version())\n",
        "    print(\"-- Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
        "\n",
        "    print(f'****** Loading pretrained pipelines fired up {datetime.now()} ****** ')\n",
        "    pipelines['ner_profiling_clinical'] = PretrainedPipeline('ner_profiling_clinical', 'en', 'clinical/models')\n",
        "    pipelines['clinical_deidentification'] = PretrainedPipeline(\"clinical_deidentification\", \"en\", \"clinical/models\")\n",
        "    event_list['pipeline_loaded'] = datetime.now()\n",
        "\n",
        "    print(event_list)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run('main:app', host='0.0.0.0', port=8515)"
      ],
      "metadata": {
        "id": "Dw4q8teuhinv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keys file"
      ],
      "metadata": {
        "id": "x06d8oc9l7TT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "...and last, but not least, add your sparknlp_keys.json to `content/sparknlp_keys.json`!"
      ],
      "metadata": {
        "id": "B_nQCoPShmvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"AWS_ACCESS_KEY_ID\": \"\",\n",
        "  \"AWS_SECRET_ACCESS_KEY\": \"\",\n",
        "  \"SECRET\": \"\",\n",
        "  \"SPARK_NLP_LICENSE\": \"\",\n",
        "  \"JSL_VERSION\": \"\",\n",
        "  \"PUBLIC_VERSION\": \"\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "CdIBuHdrhkYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building and running Docker"
      ],
      "metadata": {
        "id": "7bDRq4VkiHle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "docker build -t johnsnowlabs/sparknlp:sparknlp_api .\n",
        "docker run -v jsl_keys.json:/content/sparknlp_keys.json -p 8515:8515 -it johnsnowlabs/sparknlp:sparknlp_api\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CNjbwfPpiIkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calling the server\n",
        "`serve.py`"
      ],
      "metadata": {
        "id": "aDCAI5KriNTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "ner_text = \"\"\"\n",
        "A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus ( T2DM ), one prior episode of HTG-induced pancreatitis three years prior to presentation , associated with an acute hepatitis , and obesity with a body mass index ( BMI ) of 33.5 kg/m2 , presented with a one-week history of polyuria , polydipsia , poor appetite , and vomiting. The patient was prescribed 1 capsule of Advil 10 mg for 5 days and magnesium hydroxide 100mg/1ml suspension PO. \n",
        "He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro with meals , and metformin 1000 mg two times a day.\n",
        "\"\"\"\n",
        "\n",
        "modelname = 'clinical_deidentification'\n",
        "# modelname = 'ner_profiling_clinical'\n",
        "\n",
        "def get_url(args):\n",
        "    res = requests.get(args[0])\n",
        "    return res\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 1 call\n",
        "    # ==================\n",
        "    query = f\"?modelname={modelname}&text={ner_text}\"\n",
        "    url = f\"http://localhost:8515/benchmark/pipeline{query}\"\n",
        "    \n",
        "    print(get_url([url]))\n",
        "\n",
        "    # N calls in parallel\n",
        "    # ==================\n",
        "\n",
        "    # N_CALLS = 10    \n",
        "    # for i in range(0, N_CALLS):\n",
        "    #  list_of_urls.append((url, i))\n",
        "\n",
        "    # with ThreadPoolExecutor() as pool:\n",
        "    #  response_list = list(pool.map(get_url, list_of_urls))\n",
        "    #  print(response_list)\n",
        "    "
      ],
      "metadata": {
        "id": "Kpr1_XjniO4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the docker container is up and has finished download of the models (check `docker logs -f sparknlp_api`) you can call your server using `serve.py`"
      ],
      "metadata": {
        "id": "U4fXsYlDindo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "python serve.py\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NYUl_LFHi8AP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "synapse2",
      "language": "python",
      "name": "synapse2"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.8.12",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "Synapse example",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 4082361247410207
    },
    "colab": {
      "name": "Serving SparkNLP with API.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}