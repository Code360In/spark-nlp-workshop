{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQh3RbMfgTl2"
   },
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwURPTydgTl7"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Annotation_Lab/AnnotationLab_preannotate_upload.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EQwCxr7JGh8"
   },
   "source": [
    "# Pre-annotating and Uploading Tasks to the Annotation Lab\n",
    "\n",
    "## This tutorial provides instructions and code for the following operations:\n",
    "1. Creating Pre-annotations for the Annotation Lab\n",
    "2. Uploading Pre-annotations to the Annotation Lab\n",
    "3. Uploading Tasks Without Pre-annotations to the Annotation Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5J5QMVNSHFmf"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "license_keys = files.upload()\n",
    "\n",
    "with open(list(license_keys.keys())[0]) as f:\n",
    "    license_keys = json.load(f)\n",
    "\n",
    "# Defining license key-value pairs as local variables\n",
    "locals().update(license_keys)\n",
    "\n",
    "# Adding license key-value pairs to environment variables\n",
    "os.environ.update(license_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kilg7TU4HPSY"
   },
   "outputs": [],
   "source": [
    "# Installing pyspark and spark-nlp\n",
    "! pip install --upgrade -q pyspark==3.1.2 spark-nlp==$PUBLIC_VERSION\n",
    "\n",
    "# Installing Spark NLP Healthcare\n",
    "! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION  --extra-index-url https://pypi.johnsnowlabs.com/$SECRET\n",
    "\n",
    "# Installing Spark NLP Display Library for visualization\n",
    "! pip install -q spark-nlp-display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "_gzYc9fBVH8C",
    "outputId": "43c68f41-f05f-47bb-a1ca-b15939220f9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP Version : 3.4.4\n",
      "Spark NLP_JSL Version : 3.5.3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://7ed21addb736:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark NLP Licensed</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6869daa690>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import os\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp_jsl\n",
    "import sparknlp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "params = {\"spark.driver.memory\":\"16G\", \n",
    "          \"spark.kryoserializer.buffer.max\":\"2000M\", \n",
    "          \"spark.driver.maxResultSize\":\"2000M\"} \n",
    "\n",
    "print (\"Spark NLP Version :\", sparknlp.version())\n",
    "print (\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
    "\n",
    "spark = sparknlp_jsl.start(license_keys['SECRET'],params=params)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkkJwmEqVH8I"
   },
   "source": [
    "## 1. Creating Pre-annotations for the Annotation Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure to modify this pre-annotation pipeline to make it coherent with your Annotation Lab project configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YbMedxfWVH8J"
   },
   "outputs": [],
   "source": [
    "# Downloading sample datasets.\n",
    "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Healthcare/data/mt_samples.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "1zZg2j0dVH8J",
    "outputId": "0a106271-604e-4dd9-f5da-a5a626cb7891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sample Type / Medical Specialty:\\nHematology -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sample Type / Medical Specialty:\\nHematology -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sample Type / Medical Specialty:\\nHematology -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sample Type / Medical Specialty:\\nHematology -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sample Type / Medical Specialty:\\nHematology -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Sample Type / Medical Specialty:\\nHematology -...\n",
       "1  Sample Type / Medical Specialty:\\nHematology -...\n",
       "2  Sample Type / Medical Specialty:\\nHematology -...\n",
       "3  Sample Type / Medical Specialty:\\nHematology -...\n",
       "4  Sample Type / Medical Specialty:\\nHematology -..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = pd.read_csv('mt_samples.csv')\n",
    "print (sample_data.shape)\n",
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YLnh_EgRVH8L",
    "outputId": "ca7e64e4-38d9-4b07-96a6-5fa2799fc438"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n",
      "ner_jsl download started this may take some time.\n",
      "Approximate size to download 14.5 MB\n",
      "[OK!]\n",
      "assertion_dl download started this may take some time.\n",
      "Approximate size to download 1.3 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# pre-annotation pipeline\n",
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence = SentenceDetector()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentence')\\\n",
    "    .setCustomBounds(['\\n'])\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols([\"sentence\"])\\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "word_embeddings = WordEmbeddingsModel().pretrained('embeddings_clinical', 'en', 'clinical/models')\\\n",
    "    .setInputCols([\"sentence\", 'token'])\\\n",
    "    .setOutputCol(\"embeddings\")\\\n",
    "\n",
    "ner_model = MedicalNerModel.pretrained('ner_jsl', 'en', 'clinical/models')\\\n",
    "    .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
    "    .setOutputCol(\"ner\")\n",
    "\n",
    "converter = NerConverter()\\\n",
    "    .setInputCols([\"sentence\", \"token\", \"ner\"])\\\n",
    "    .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "assertion_model = AssertionDLModel().pretrained('assertion_dl', 'en', 'clinical/models')\\\n",
    "    .setInputCols([\"sentence\", \"ner_chunk\", 'embeddings'])\\\n",
    "    .setOutputCol(\"assertion_res\")\n",
    "\n",
    "ner_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        sentence,\n",
    "        tokenizer,\n",
    "        word_embeddings,\n",
    "        ner_model,\n",
    "        converter,\n",
    "        assertion_model\n",
    "    ])\n",
    "\n",
    "empty_data = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "pipeline_model = ner_pipeline.fit(empty_data)\n",
    "lmodel = LightPipeline(pipeline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R_GQtfPJVH8N",
    "outputId": "b219cd72-52ae-499b-afb3-c2b8409281a4"
   },
   "outputs": [],
   "source": [
    "# pre-annotate dataframe using the pipeline above\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def get_preannotations_from_NER (list_of_files, ner_prediction_model):\n",
    "    \n",
    "    print(len(list_of_files), \" documents will be preannotated ...\")\n",
    "    \n",
    "    file_text_tuples = []\n",
    "    for index, file_text in enumerate(list_of_files):\n",
    "        ## \n",
    "        file_text_tuples.append((index, # id of the file\n",
    "                                 'demo_mt_samples_{}'.format(index), # this is the title that appears on the UI\n",
    "                                 file_text # text of the file\n",
    "                                ))\n",
    "        \n",
    "    # Define schema\n",
    "    schema = StructType([\n",
    "        StructField(\"task_id\", StringType(), True),\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"text\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    # Create dataframe\n",
    "    \n",
    "    spark_df = spark.createDataFrame(file_text_tuples, schema)\n",
    "    \n",
    "    print(\"created spark dataframe ... transforming started\")\n",
    "        \n",
    "    pred_df = ner_prediction_model.transform(spark_df)\n",
    "    \n",
    "    print(\"pandas conversion started\")\n",
    "    \n",
    "    view_df = pred_df.select(\"task_id\",\n",
    "                             'title', \n",
    "                             \"text\",\n",
    "                             \"ner_chunk\", ## you can change this to any column name (the final chunk column in your pp)\n",
    "                             \"assertion_res\" # - if you want assertion annotations as well\n",
    "                            ).toPandas()\n",
    "    \n",
    "        \n",
    "    view_df['task_id']=view_df['task_id'].astype(int)\n",
    "    \n",
    "    return view_df\n",
    "\n",
    "preds_df = get_preannotations_from_NER(sample_data['text'].values, pipeline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZK3OHDtIVH8O",
    "outputId": "1e16df7a-9759-4672-aada-f5d2b7a19292"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations payload is ready\n",
      "Annotated Documents: 50\n"
     ]
    }
   ],
   "source": [
    "# convert pre-annotations to JSON format for uploading to Annotation Lab\n",
    "import random as rand\n",
    "import datetime\n",
    "\n",
    "def generate_hash(length=10):    \n",
    "    nums = list(range(48,58))\n",
    "    uppers = list(range(65,91))\n",
    "    lowers = list(range(97,123))\n",
    "    all_chars = nums+uppers+lowers\n",
    "    return \"\".join([chr(all_chars[rand.randint(0, len(all_chars)-1)]) for x in range(length)])\n",
    "\n",
    "def create_import_json (username, pandas_pred_df, project_id):\n",
    "    \n",
    "    def build_label(chunk, start, end, label):\n",
    "        \n",
    "        label_json = {\n",
    "                \"from_name\": \"label\",\n",
    "                \"id\": generate_hash(),\n",
    "                \"source\": \"$text\",\n",
    "                \"to_name\": \"text\",\n",
    "                \"type\": \"labels\",\n",
    "                \"value\": {\n",
    "                  \"end\": end,\n",
    "                  \"labels\": [label],\n",
    "                  \"start\": start,\n",
    "                  \"text\": chunk\n",
    "                }\n",
    "              }\n",
    "        return label_json\n",
    "\n",
    "    import_json = []\n",
    "\n",
    "    for i,row in pandas_pred_df.iterrows():\n",
    "       \n",
    "        results_jsons = [] \n",
    "        \n",
    "        assertion_mapper = {}\n",
    "        for x in row[\"ner_chunk\"]: # assign proper column name\n",
    "            if not pd.isna(x):\n",
    "                results_jsons.append(build_label(x.result, x.begin, x.end+1, x.metadata[\"entity\"]))\n",
    "                assertion_mapper[x.begin] = x.result\n",
    "                \n",
    "        # comment out this loop if assertion is not required\n",
    "        for x in row[\"assertion_res\"]:\n",
    "            if not pd.isna(x):\n",
    "                results_jsons.append(build_label(assertion_mapper[x.begin], x.begin, x.end+1, x.result))\n",
    "                \n",
    "             \n",
    "        import_json.append({\"predictions\": [{\n",
    "            'created_username': username,\n",
    "                \"result\":results_jsons\n",
    "            }],\n",
    "            \"data\":{\n",
    "                \"text\":row[\"text\"],\n",
    "                \"title\":row['title']\n",
    "            },\n",
    "                            'id':row['task_id']\n",
    "                           })\n",
    "    \n",
    "    print(\"Annotations payload is ready\")\n",
    "    \n",
    "    return import_json\n",
    "\n",
    "annotation_json = create_import_json('ner_jsl', preds_df, 'demo_100')\n",
    "\n",
    "print('Annotated Documents:' , len(annotation_json))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Uploading Pre-annotations to the Annotation Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by providing your user credentials and making sure this `get_cookies` function returns a `200` response code. The base url for this demo is: https://annotationlab.johnsnowlabs.com - make sure to change this accordingly. The code blocks below are based on the Annotation Lab [API documentation](https://nlp.johnsnowlabs.com/docs/en/alab/api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGQqgAIRVH8I",
    "outputId": "584ab7b6-b60c-4a8d-bb61-77aa688b2fff",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# provide user credentials\n",
    "username = 'user'\n",
    "password = 'pass'\n",
    "client_secret = \"secret\"\n",
    "\n",
    "# helper function to get cookies\n",
    "def get_cookies(username, password):\n",
    "    \n",
    "    url = \"https://annotationlab.johnsnowlabs.com/openid-connect/token\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"accept\": \"*/*\",\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "      \"username\": username,\n",
    "      \"password\": password,\n",
    "      \"client_id\": \"annotator\",\n",
    "      \"client_secret\": client_secret\n",
    "    }\n",
    "    \n",
    "    resp = requests.post(url, headers=headers, json=data)\n",
    "    print (resp.status_code)\n",
    "    auth_info = resp.json()\n",
    "\n",
    "    cookies = {\n",
    "        'access_token': f\"Bearer {auth_info['access_token']}\",\n",
    "        'refresh_token': auth_info['refresh_token']\n",
    "    }\n",
    "    return cookies\n",
    "\n",
    "cookies = get_cookies(username, password)\n",
    "cookies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the name of your project, making sure it is the official name used in the Annotation Lab which is accessible by the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "pfdZaAvHVH8O",
    "outputId": "40a96d64-49ec-4a1a-b8b7-280c2c385cf9"
   },
   "outputs": [],
   "source": [
    "project_name = 'demo_100'\n",
    "url = \"https://annotationlab.johnsnowlabs.com/api/projects/{}/import\".format(project_name)\n",
    "print(url)\n",
    "\n",
    "headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"accept\": \"*/*\"\n",
    "    }\n",
    "\n",
    "cookies = get_cookies(username, password)\n",
    "\n",
    "resp = requests.post(url, headers=headers, cookies=cookies, json=annotation_json)\n",
    "\n",
    "resp.status_code\n",
    "resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV6VBTQMVH8P"
   },
   "source": [
    "## 3. Uploading Tasks Without Pre-annotations to the Annotation Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by providing your user credentials and making sure this `get_cookies` function returns a `200` response code. The base url for this demo is: https://annotationlab.johnsnowlabs.com - make sure to change this accordingly. The code blocks below are based on the Annotation Lab [API documentation](https://nlp.johnsnowlabs.com/docs/en/alab/api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGQqgAIRVH8I",
    "outputId": "584ab7b6-b60c-4a8d-bb61-77aa688b2fff",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# provide user credentials\n",
    "username = 'user'\n",
    "password = 'pass'\n",
    "client_secret = \"secret\"\n",
    "\n",
    "# helper function to get cookies\n",
    "def get_cookies(username, password):\n",
    "    \n",
    "    url = \"https://annotationlab.johnsnowlabs.com/openid-connect/token\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"accept\": \"*/*\",\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "      \"username\": username,\n",
    "      \"password\": password,\n",
    "      \"client_id\": \"annotator\",\n",
    "      \"client_secret\": client_secret\n",
    "    }\n",
    "    \n",
    "    resp = requests.post(url, headers=headers, json=data)\n",
    "    print (resp.status_code)\n",
    "    auth_info = resp.json()\n",
    "\n",
    "    cookies = {\n",
    "        'access_token': f\"Bearer {auth_info['access_token']}\",\n",
    "        'refresh_token': auth_info['refresh_token']\n",
    "    }\n",
    "    return cookies\n",
    "\n",
    "cookies = get_cookies(username, password)\n",
    "cookies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this tutorial, we will be uploading the text samples from this csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YbMedxfWVH8J"
   },
   "outputs": [],
   "source": [
    "# Downloading sample datasets.\n",
    "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Healthcare/data/mt_samples.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "1zZg2j0dVH8J",
    "outputId": "0a106271-604e-4dd9-f5da-a5a626cb7891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sample Type / Medical Specialty:\\nHematology -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sample Type / Medical Specialty:\\nHematology -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sample Type / Medical Specialty:\\nHematology -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sample Type / Medical Specialty:\\nHematology -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sample Type / Medical Specialty:\\nHematology -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Sample Type / Medical Specialty:\\nHematology -...\n",
       "1  Sample Type / Medical Specialty:\\nHematology -...\n",
       "2  Sample Type / Medical Specialty:\\nHematology -...\n",
       "3  Sample Type / Medical Specialty:\\nHematology -...\n",
       "4  Sample Type / Medical Specialty:\\nHematology -..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = pd.read_csv('mt_samples.csv')\n",
    "print (sample_data.shape)\n",
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "yjXuypwHVH8P",
    "outputId": "0c908709-1a90-44ea-f58e-eb77d07cd03b"
   },
   "outputs": [],
   "source": [
    "def create_sample_data(text_list):\n",
    "    sample_data_for_upload = []\n",
    "    for index, text in enumerate(text_list):\n",
    "        sample_data_for_upload.append({'title': index, 'text': text})\n",
    "\n",
    "    return sample_data_for_upload\n",
    "\n",
    "sample_data_for_upload = create_sample_data(sample_data['text'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the name of your project, making sure it is the official name used in the Annotation Lab which is accessible by the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'demo_100'\n",
    "url = \"https://annotationlab.johnsnowlabs.com/api/projects/{}/import\".format(project_name)\n",
    "print(url)\n",
    "\n",
    "headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"accept\": \"*/*\"\n",
    "    }\n",
    "\n",
    "cookies = get_cookies(username, password)\n",
    "\n",
    "resp = requests.post(url, headers=headers, cookies=cookies, json=sample_data_for_upload)\n",
    "\n",
    "resp.status_code\n",
    "resp.text"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AL_API_import_export_pre_annotate.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "luca_base_400",
   "language": "python",
   "name": "luca_base_400"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
