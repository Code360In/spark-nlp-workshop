{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQh3RbMfgTl2"
      },
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwURPTydgTl7"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Annotation_Lab/AL_API_import_export_pre_annotate.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EQwCxr7JGh8"
      },
      "source": [
        "# Connect to Annotation Lab via API.\n",
        "## This tutorial provides instrudctions and code for the following operations:\n",
        "- Uploading Pre-annotations to Alab\n",
        "- Importing a project from Alab, and converting to get conll, Assertion files.\n",
        "- Uploading tasks without pre-annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J5QMVNSHFmf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "license_keys = files.upload()\n",
        "\n",
        "with open(list(license_keys.keys())[0]) as f:\n",
        "    license_keys = json.load(f)\n",
        "\n",
        "# Defining license key-value pairs as local variables\n",
        "locals().update(license_keys)\n",
        "\n",
        "# Adding license key-value pairs to environment variables\n",
        "os.environ.update(license_keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kilg7TU4HPSY"
      },
      "outputs": [],
      "source": [
        "# Installing pyspark and spark-nlp\n",
        "! pip install --upgrade -q pyspark==3.1.2 spark-nlp==$PUBLIC_VERSION\n",
        "\n",
        "# Installing Spark NLP Healthcare\n",
        "! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION  --extra-index-url https://pypi.johnsnowlabs.com/$SECRET\n",
        "\n",
        "# Installing Spark NLP Display Library for visualization\n",
        "! pip install -q spark-nlp-display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "_gzYc9fBVH8C",
        "outputId": "43c68f41-f05f-47bb-a1ca-b15939220f9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP Version : 3.4.4\n",
            "Spark NLP_JSL Version : 3.5.3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f6869daa690>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://7ed21addb736:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP Licensed</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "from zipfile import ZipFile\n",
        "from io import BytesIO\n",
        "import os\n",
        "from pyspark.ml import Pipeline,PipelineModel\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp_jsl.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp_jsl\n",
        "import sparknlp\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "params = {\"spark.driver.memory\":\"16G\", \n",
        "          \"spark.kryoserializer.buffer.max\":\"2000M\", \n",
        "          \"spark.driver.maxResultSize\":\"2000M\"} \n",
        "\n",
        "print (\"Spark NLP Version :\", sparknlp.version())\n",
        "print (\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
        "\n",
        "spark = sparknlp_jsl.start(license_keys['SECRET'],params=params)\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIijfVRHVH8G"
      },
      "source": [
        "**Note: The base url for this demo is: https://annotationlab.johnsnowlabs.com - you can change this accordingly**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7RkDZKNbEsX"
      },
      "source": [
        "**Provide you user credentials**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHcR4IBqa_tA"
      },
      "outputs": [],
      "source": [
        "username = 'user'\n",
        "password = 'pass'\n",
        "client_secret = \"secret\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql-Hs0yzVH8H"
      },
      "source": [
        "**Helper Function to get cookies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGQqgAIRVH8I",
        "outputId": "584ab7b6-b60c-4a8d-bb61-77aa688b2fff",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200\n"
          ]
        }
      ],
      "source": [
        "def get_cookies(username, password):\n",
        "    \n",
        "    \n",
        "    url = \"https://annotationlab.johnsnowlabs.com/openid-connect/token\"\n",
        "    \n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"accept\": \"*/*\",\n",
        "    }\n",
        "    \n",
        "    data = {\n",
        "      \"username\": username,\n",
        "      \"password\": password,\n",
        "      \"client_id\": \"annotator\",\n",
        "      \"client_secret\": client_secret\n",
        "    }\n",
        "    \n",
        "    resp = requests.post(url, headers=headers, json=data)\n",
        "    print (resp.status_code)\n",
        "    auth_info = resp.json()\n",
        "\n",
        "    cookies = {\n",
        "        'access_token': f\"Bearer {auth_info['access_token']}\",\n",
        "        'refresh_token': auth_info['refresh_token']\n",
        "    }\n",
        "    return cookies\n",
        "\n",
        "cookies = get_cookies(username, password)\n",
        "#cookies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkkJwmEqVH8I"
      },
      "source": [
        "# Download sample data for uploading to Alab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbMedxfWVH8J"
      },
      "outputs": [],
      "source": [
        "# Downloading sample datasets.\n",
        "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Healthcare/data/mt_samples.csv\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "1zZg2j0dVH8J",
        "outputId": "0a106271-604e-4dd9-f5da-a5a626cb7891"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50, 1)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sample Type / Medical Specialty:\\nHematology -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sample Type / Medical Specialty:\\nHematology -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sample Type / Medical Specialty:\\nHematology -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sample Type / Medical Specialty:\\nHematology -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sample Type / Medical Specialty:\\nHematology -...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  Sample Type / Medical Specialty:\\nHematology -...\n",
              "1  Sample Type / Medical Specialty:\\nHematology -...\n",
              "2  Sample Type / Medical Specialty:\\nHematology -...\n",
              "3  Sample Type / Medical Specialty:\\nHematology -...\n",
              "4  Sample Type / Medical Specialty:\\nHematology -..."
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_data = pd.read_csv('mt_samples.csv')\n",
        "print (sample_data.shape)\n",
        "sample_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSZVXEZAVH8K"
      },
      "source": [
        "# 1. Pre-annotate, and upload to a project on Alab\n",
        "\n",
        "**Note: Your project configuration should be coherent with your pre-annotation pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXqhEvreVH8L"
      },
      "source": [
        "**1.1 Pipeline for pre-annotation. You can change according to requirements.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLnh_EgRVH8L",
        "outputId": "ca7e64e4-38d9-4b07-96a6-5fa2799fc438"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings_clinical download started this may take some time.\n",
            "Approximate size to download 1.6 GB\n",
            "[OK!]\n",
            "ner_jsl download started this may take some time.\n",
            "Approximate size to download 14.5 MB\n",
            "[OK!]\n",
            "assertion_dl download started this may take some time.\n",
            "Approximate size to download 1.3 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "document = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "sentence = SentenceDetector()\\\n",
        "    .setInputCols(['document'])\\\n",
        "    .setOutputCol('sentence')\\\n",
        "    .setCustomBounds(['\\n'])\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"sentence\"]) \\\n",
        "    .setOutputCol(\"token\")\\\n",
        "    .setSplitChars(['\\[','\\]'])\\\n",
        "    .setContextChars([\".\", \",\", \";\", \":\", \"!\", \"?\", \"*\", \"-\", \"(\", \")\", \"\\\"\", \"'\",\"+\",\"%\",\"-\"])\n",
        "\n",
        "word_embeddings = WordEmbeddingsModel().pretrained('embeddings_clinical', 'en', 'clinical/models')\\\n",
        "    .setInputCols([\"sentence\", 'token']) \\\n",
        "    .setOutputCol(\"embeddings\")\\\n",
        "\n",
        "ner_model = MedicalNerModel.pretrained('ner_jsl', 'en', 'clinical/models')\\\n",
        "    .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
        "    .setOutputCol(\"ner\")\n",
        "\n",
        "converter = NerConverter()\\\n",
        "    .setInputCols([\"sentence\", \"token\", \"ner\"])\\\n",
        "    .setOutputCol(\"ner_chunk\")\n",
        "\n",
        "assertion_model = AssertionDLModel().pretrained('assertion_dl', 'en', 'clinical/models')\\\n",
        "    .setInputCols([\"sentence\", \"ner_chunk\", 'embeddings'])\\\n",
        "    .setOutputCol(\"assertion_res\")\n",
        "\n",
        "ner_pipeline = Pipeline(\n",
        "    stages = [\n",
        "        document,\n",
        "        sentence,\n",
        "        tokenizer,\n",
        "        word_embeddings,\n",
        "        ner_model,\n",
        "        converter,\n",
        "        assertion_model\n",
        "    ])\n",
        "\n",
        "empty_data = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "pipeline_model = ner_pipeline.fit(empty_data)\n",
        "lmodel = LightPipeline(pipeline_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zreCQgDFVH8M"
      },
      "source": [
        "**1.2 Get Pre-Annotations using the pipeline above and convert to required format**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_GQtfPJVH8N",
        "outputId": "b219cd72-52ae-499b-afb3-c2b8409281a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50  documents will be preannotated ...\n",
            "created spark dataframe ... transforming started\n",
            "pandas conversion started\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "def get_preannotations_from_NER (list_of_files, ner_prediction_model):\n",
        "    \n",
        "    print (len(list_of_files), \" documents will be preannotated ...\")\n",
        "    \n",
        "    file_text_tuples = []\n",
        "    for index, file_text in enumerate(list_of_files):\n",
        "        ## \n",
        "        file_text_tuples.append((index, # id of the file\n",
        "                                 'demo_mt_samples_{}'.format(index), # this is the title that appears on the UI\n",
        "                                 file_text # text of the file\n",
        "                                ))\n",
        "        \n",
        "    # Define schema\n",
        "    schema = StructType([\n",
        "        StructField(\"task_id\", StringType(), True),\n",
        "        StructField(\"title\", StringType(), True),\n",
        "        StructField(\"text\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "    # Create dataframe\n",
        "    \n",
        "    spark_df = spark.createDataFrame(file_text_tuples, schema)\n",
        "    \n",
        "    print (\"created spark dataframe ... transforming started\")\n",
        "        \n",
        "    pred_df = ner_prediction_model.transform(spark_df)\n",
        "    \n",
        "    print (\"pandas conversion started\")\n",
        "    \n",
        "    view_df = pred_df.select(\"task_id\",\n",
        "                             'title', \n",
        "                             \"text\",\n",
        "                             \"ner_chunk\", ## you can change this to any column name (the final chunk column in your pp)\n",
        "                             \"assertion_res\" # - if you want assertion annotations as well\n",
        "                            ).toPandas()\n",
        "    \n",
        "        \n",
        "    view_df['task_id']=view_df['task_id'].astype(int)\n",
        "    \n",
        "    return view_df\n",
        "\n",
        "preds_df = get_preannotations_from_NER(sample_data['text'].values, pipeline_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC0tuOo6VH8N"
      },
      "source": [
        "**1.3 Prepare the JSON to upload to Alab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK3OHDtIVH8O",
        "outputId": "1e16df7a-9759-4672-aada-f5d2b7a19292"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Annotations payload is ready\n",
            "Annotated Documents: 50\n"
          ]
        }
      ],
      "source": [
        "import random as rand\n",
        "import datetime\n",
        "\n",
        "def generate_hash(length=10):    \n",
        "    nums = list(range(48,58))\n",
        "    uppers = list(range(65,91))\n",
        "    lowers = list(range(97,123))\n",
        "    all_chars = nums+uppers+lowers\n",
        "    return \"\".join([chr(all_chars[rand.randint(0, len(all_chars)-1)]) for x in range(length)])\n",
        "\n",
        "def create_import_json (username, pandas_pred_df, project_id):\n",
        "    \n",
        "    def build_label(chunk, start, end, label):\n",
        "        \n",
        "        label_json = {\n",
        "                \"from_name\": \"label\",\n",
        "                \"id\": generate_hash(),\n",
        "                \"source\": \"$text\",\n",
        "                \"to_name\": \"text\",\n",
        "                \"type\": \"labels\",\n",
        "                \"value\": {\n",
        "                  \"end\": end,\n",
        "                  \"labels\": [label],\n",
        "                  \"start\": start,\n",
        "                  \"text\": chunk\n",
        "                }\n",
        "              }\n",
        "        return label_json\n",
        "\n",
        "    import_json = []\n",
        "\n",
        "    for i,row in pandas_pred_df.iterrows():\n",
        "       \n",
        "        results_jsons = [] \n",
        "        \n",
        "        assertion_mapper = {}\n",
        "        for x in row[\"ner_chunk\"]: # assign proper column name\n",
        "            if not pd.isna(x):\n",
        "                results_jsons.append(build_label(x.result, x.begin, x.end+1, x.metadata[\"entity\"]))\n",
        "                assertion_mapper[x.begin] = x.result\n",
        "                \n",
        "        # comment out this loop if assertion is not required\n",
        "        for x in row[\"assertion_res\"]:\n",
        "            if not pd.isna(x):\n",
        "                results_jsons.append(build_label(assertion_mapper[x.begin], x.begin, x.end+1, x.result))\n",
        "                \n",
        "             \n",
        "        import_json.append({\"predictions\": [{\n",
        "            'created_username': username,\n",
        "                \"result\":results_jsons\n",
        "            }],\n",
        "            \"data\":{\n",
        "                \"text\":row[\"text\"],\n",
        "                \"title\":row['title']\n",
        "            },\n",
        "                            'id':row['task_id']\n",
        "                           })\n",
        "    \n",
        "    print (\"Annotations payload is ready\")\n",
        "    \n",
        "    return import_json\n",
        "\n",
        "annotation_json = create_import_json('ner_jsl', preds_df, 'demo_100')\n",
        "\n",
        "print ('Annotated Documents:' , len(annotation_json))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RcutO2iVH8O"
      },
      "source": [
        "**1.4 Upload pre-annotations to Alab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "pfdZaAvHVH8O",
        "outputId": "40a96d64-49ec-4a1a-b8b7-280c2c385cf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://annotationlab.johnsnowlabs.com/api/projects/demo_100/import\n",
            "200\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{\"completion_count\":0,\"duration\":6.411985635757446,\"failed_count\":0,\"ignored_count\":0,\"prediction_count\":50,\"task_count\":50,\"task_ids\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"task_title_warning\":0,\"updated_count\":0}\\n'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "project_name = 'demo_100'\n",
        "url = \"https://annotationlab.johnsnowlabs.com/api/projects/{}/import\".format(project_name)\n",
        "print (url)\n",
        "\n",
        "headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"accept\": \"*/*\"\n",
        "    }\n",
        "\n",
        "cookies = get_cookies(username, password)\n",
        "\n",
        "resp = requests.post(url, headers = headers, cookies = cookies, json = annotation_json)\n",
        "\n",
        "resp.status_code\n",
        "resp.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNRDzyhmVH8P"
      },
      "source": [
        "# 2. Download / Export a project as json from Alab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NCcKmLqhEAa"
      },
      "source": [
        "**2.1 Export project from Alab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnNVex2FVH8P",
        "outputId": "8d5c5ce3-0923-432b-d768-22062d6f84de",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://annotationlab.johnsnowlabs.com/api/projects/demo_100/export?format=JSON\n",
            "200\n",
            "Total tasks in the project with completions: 4\n"
          ]
        }
      ],
      "source": [
        "project_name = 'demo_100'\n",
        "url = \"https://annotationlab.johnsnowlabs.com/api/projects/{}/export?format=JSON\".format(project_name)\n",
        "print (url)\n",
        "\n",
        "headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"accept\": \"*/*\"\n",
        "    }\n",
        "\n",
        "cookies = get_cookies(username, password)\n",
        "\n",
        "resp = requests.post(url, headers = headers, cookies = cookies)\n",
        "\n",
        "zipfile = ZipFile(BytesIO(resp.content))\n",
        "with zipfile.open(zipfile.namelist()[0]) as f:  \n",
        "    data = f.read()  \n",
        "project_json = json.loads(data)  \n",
        "\n",
        "print ('Total tasks in the project with completions:', len(project_json))\n",
        "\n",
        "with open('project_export.json', 'w') as f_:\n",
        "    f_.write(json.dumps(project_json, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93UnENybjKH7"
      },
      "source": [
        "**2.1 Parse the project json and generate conll file**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import *\n",
        "from sparknlp_jsl.annotator import *\n",
        "from sparknlp.base import *\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "\n",
        "def get_nlp_pipeline():\n",
        "\n",
        "    documentAssembler = DocumentAssembler()\\\n",
        "      .setInputCol(\"text\")\\\n",
        "      .setOutputCol(\"document\")\n",
        "\n",
        "    sentenceDetector = SentenceDetectorDLModel.pretrained(\"sentence_detector_dl_healthcare\",\"en\",\"clinical/models\")\\\n",
        "      .setInputCols([\"document\"])\\\n",
        "      .setOutputCol(\"sentence\")\n",
        "    \n",
        "    pattern = \"\\\\s+|(?=[-.:;*+,$&%\\\\[\\\\]\\\\(\\\\)\\\\/])|(?<=[-.:;*+,$&%\\\\[\\\\]\\\\(\\\\)\\\\/])\"\n",
        "    \n",
        "    regex_tokenizer = RegexTokenizer() \\\n",
        "        .setInputCols([\"sentence\"]) \\\n",
        "        .setOutputCol(\"token\")\\\n",
        "        .setPositionalMask(True)\\\n",
        "        .setPattern(pattern)\n",
        "        \n",
        "    pos = PerceptronModel.pretrained(\"pos_clinical\",\"en\",\"clinical/models\")\\\n",
        "        .setInputCols([\"sentence\", \"token\"]) \\\n",
        "        .setOutputCol(\"pos\")\n",
        "\n",
        "    pipeline = Pipeline(\n",
        "        stages = [\n",
        "            documentAssembler,\n",
        "            sentenceDetector,\n",
        "            regex_tokenizer,\n",
        "            pos]\n",
        "    )\n",
        "\n",
        "    empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "\n",
        "    pipelineFit = pipeline.fit(empty_data)\n",
        "\n",
        "    lp_pipeline = LightPipeline(pipelineFit)\n",
        "    \n",
        "    print (\"Spark NLP lightpipeline is created\")\n",
        "    \n",
        "    return lp_pipeline\n",
        "\n",
        "lp_pipeline =  get_nlp_pipeline()\n",
        "\n",
        "def get_token_pipeline():\n",
        "\n",
        "    documentAssembler = DocumentAssembler()\\\n",
        "      .setInputCol(\"text\")\\\n",
        "      .setOutputCol(\"sentence\")\n",
        "    \n",
        "    pattern = \"\\\\s+|(?=[-.:;*+,$&%\\\\[\\\\]\\\\(\\\\)\\\\/])|(?<=[-.:;*+,$&%\\\\[\\\\]\\\\(\\\\)\\\\/])\"\n",
        "    \n",
        "    regex_tokenizer = RegexTokenizer() \\\n",
        "        .setInputCols([\"sentence\"]) \\\n",
        "        .setOutputCol(\"token\")\\\n",
        "        .setPositionalMask(True)\\\n",
        "        .setPattern(pattern)\n",
        "\n",
        "    pipeline = Pipeline(\n",
        "        stages = [\n",
        "            documentAssembler,\n",
        "            regex_tokenizer]\n",
        "    )\n",
        "\n",
        "    empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "\n",
        "    pipelineFit = pipeline.fit(empty_data)\n",
        "\n",
        "    lp_pipeline = LightPipeline(pipelineFit)\n",
        "    \n",
        "    print(\"Spark NLP lightpipeline is created\")\n",
        "    \n",
        "    return lp_pipeline\n",
        "\n",
        "token_lp_pipeline =  get_token_pipeline()"
      ],
      "metadata": {
        "id": "W6jw9MRtrMe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_conll_manual(lp_pipeline, output, excludedDocs=[''], \n",
        "                     excluded=['present', 'absent', 'possbile', 'hypothetical', 'conditional', 'associated_with_someone_else'], # set all assertions here\n",
        "                     included_phrase=''):\n",
        "    \n",
        "    conll_lines=[]\n",
        "\n",
        "    try:\n",
        "        new_content = output['data']['text']\n",
        "    except:\n",
        "        new_content =  output['data']['longText']\n",
        "    \n",
        "    try:\n",
        "        title = output['data']['title']\n",
        "    except:\n",
        "        title = \"task_id\" + str(output['id'])\n",
        "    print(title)\n",
        "    n = lp_pipeline.fullAnnotate(new_content)\n",
        "\n",
        "    sent_tuples = {str(x.metadata['sentence']): (x.begin, x.end) for x in n[0]['sentence']}\n",
        "    parsed = [(int(x.metadata['sentence']), x.result, x.begin, x.end, y.result, sent_tuples[x.metadata['sentence']][0]) for x,y in zip(n[0][\"token\"],n[0][\"pos\"])]\n",
        "\n",
        "    ents = []\n",
        "    \n",
        "    if len(output['completions'])!=0 and title.strip() not in excludedDocs:\n",
        "\n",
        "        ann_results = output['completions'][-1]['result']\n",
        "\n",
        "        for d in ann_results: \n",
        "            #and included_phrase in d['value']['labels'][0] \n",
        "            if d['type']=='labels' and \\\n",
        "            d['value']['labels'][0].replace(' ','') not in excluded:                \n",
        "                \n",
        "                temp_text = d['value']['text']\n",
        "                start=d['value']['start']\n",
        "                end=d['value']['end']\n",
        "\n",
        "                if len(temp_text)!=len(temp_text.rstrip()):\n",
        "                    end = end-(len(temp_text)-len(temp_text.rstrip()))\n",
        "                    temp_text = temp_text.rstrip()\n",
        "\n",
        "                if len(temp_text)!=len(temp_text.lstrip()):\n",
        "                    start = start+(len(temp_text)-len(temp_text.lstrip())) \n",
        "                    temp_text = temp_text.lstrip()\n",
        "\n",
        "                ents.append((temp_text, d['value']['labels'][0].replace(' ',''), start, end))\n",
        "\n",
        "        text_df = pd.DataFrame(ents, columns=['chunk','label','start','end']) \n",
        "        \n",
        "        text_df['label'] = text_df['label'].apply(lambda x: x.replace(' ',''))   \n",
        "        \n",
        "        text_df.sort_values(by=['start', 'end'], inplace=True)\n",
        "        text_df.reset_index(drop=True, inplace=True)\n",
        "        \n",
        "        df = text_df.copy()\n",
        "        \n",
        "        tag_dict = {}\n",
        "        \n",
        "\n",
        "        for i,row in df.iterrows():\n",
        "                               \n",
        "            base_ix= row[\"start\"]\n",
        "            \n",
        "            chunk_tokens = token_lp_pipeline.fullAnnotate(row['chunk'])[0]['token']\n",
        "\n",
        "            for i, token in enumerate(chunk_tokens):\n",
        "                if i == 0:\n",
        "                    iob = 'B-'\n",
        "                else:\n",
        "                    iob = 'I-'\n",
        "                tag_dict[(base_ix+token.begin, token.result)] = iob + row['label'].replace(' ','')  \n",
        "                \n",
        "        s=0\n",
        "        for i, p in enumerate(parsed):\n",
        "            if p[0]!=s:\n",
        "                conll_lines.append(\"\\n\")\n",
        "                s+=1\n",
        "            conll_lines.append(\"{} {} {} {}\\n\".format(p[1], p[4], p[4], tag_dict.get((p[2]+p[-1],p[1]),\"O\")))\n",
        "\n",
        "        conll_lines.append(\"\\n\")\n",
        "\n",
        "        return conll_lines\n",
        "    \n",
        "    else:\n",
        "       \n",
        "        return ''\n",
        "\n",
        "\n",
        "def get_conll_from_json_manual(pid, bulk_json_file_path):\n",
        "    \n",
        "    with open(bulk_json_file_path, 'r') as json_file:\n",
        "        \n",
        "        json_outputs = json.load(json_file)\n",
        "    \n",
        "    bulk_conll_lines = [\"-DOCSTART- -X- -X- O\\n\\n\"]\n",
        "\n",
        "    print (len(json_outputs))\n",
        "    \n",
        "    for o, output in enumerate(json_outputs):\n",
        "\n",
        "        in_conll_lines = get_conll_manual(lp_pipeline, output) \n",
        "        \n",
        "        bulk_conll_lines.extend(in_conll_lines)\n",
        "        \n",
        "        print (o)        \n",
        "        \n",
        "    try:\n",
        "        os.mkdir('exported_conlls')\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    with open('exported_conlls/project_{}.conll'.format(pid), 'w', encoding='utf-8') as f:\n",
        "        for i in bulk_conll_lines:\n",
        "            f.write(i)\n",
        "\n",
        "    print ('exported_conlls/project_{}.conll'.format(pid))\n",
        "    \n",
        "    return bulk_conll_lines"
      ],
      "metadata": {
        "id": "V0E1ByEWra-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_conll_from_json_manual('demo_100','./project_export.json')"
      ],
      "metadata": {
        "id": "Dh0AnpH4syZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWgJvL7GjbAH"
      },
      "source": [
        "**2.2 Generate Data for Training Assertion Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8chf7-dgGFl"
      },
      "outputs": [],
      "source": [
        "from sparknlp_jsl.training import *\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "json_path = './project_export.json'\n",
        "\n",
        "rdr = AnnotationToolJsonReader(assertion_labels = ['present', 'absent', 'possbile', 'hypothetical', 'conditional', 'associated_with_someone_else'])\n",
        "\n",
        "df_anns = rdr.readDataset(spark, json_path).withColumn(\"json\",F.lit(json_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43eUjt5akJ-0"
      },
      "outputs": [],
      "source": [
        "rel_pds = df_anns.toPandas()\n",
        "print (rel_pds.shape)\n",
        "print (rel_pds['task_id'].nunique())\n",
        "rel_pds = rel_pds.sort_values('completion_id').drop_duplicates('task_id', keep='last')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e97vwcDYjZuH",
        "outputId": "ec70673c-f38a-4d61-8ddd-afc366985053"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "present                         634\n",
            "absent                          300\n",
            "hypothetical                     48\n",
            "associated_with_someone_else     36\n",
            "conditional                      11\n",
            "Name: assertion_label, dtype: int64\n",
            "(1029, 12)\n"
          ]
        }
      ],
      "source": [
        "rel_pds = rel_pds[rel_pds['assertion_label'].apply(lambda x: len(x)) > 0]\n",
        "\n",
        "all_tasks_assertions = []\n",
        "all_tasks_relations = []\n",
        "for index, group in rel_pds.groupby('task_id'):\n",
        "    \n",
        "    print (index)\n",
        "    \n",
        "    ann_chunks = pd.DataFrame( [{'chunk_id': i.metadata['chunk_id'], 'chunk': i.result, 'begin': int(i.begin), 'end': i.end, 'entity': i.metadata['entity']} for i in group['tool_chunk'].explode() ] )\n",
        "        \n",
        "    ner_chunks = pd.DataFrame( [{'chunk_num': ii, 'chunk': i.result, 'begin': int(i.begin), 'end': i.end, 'sentence_id': int(i.metadata['sentence'])} for ii, i in enumerate(group['ner_chunk'].explode()) ] )\n",
        "    \n",
        "    with_sent = pd.merge(ann_chunks[['chunk_id', 'begin', 'entity']], ner_chunks, on=['begin'], how='inner')\n",
        "    \n",
        "    sentences_df = pd.DataFrame( [{'sentence_id': int(i.metadata['sentence']), 'sentence': i.result, 'sent_begin': int(i.begin), 'sent_end': int(i.end)} for i in group['sentence'].explode() ] )\n",
        "    \n",
        "    with_sent = pd.merge(with_sent, sentences_df, on=['sentence_id'], how='inner')\n",
        "\n",
        "    assertion_chunks = pd.DataFrame( [{'assertion_label': i.result, 'begin': int(i.begin) } for i in group['assertion_label'].explode() ] )\n",
        "    \n",
        "    assertion_chunks = pd.merge(assertion_chunks, with_sent, on=['begin'], how='inner')\n",
        "    \n",
        "    assertion_chunks['task_id'] = index\n",
        "    all_tasks_assertions.append(assertion_chunks)\n",
        "        \n",
        "all_tasks_assertions = pd.concat(all_tasks_assertions, axis=0)\n",
        "print (all_tasks_assertions['assertion_label'].value_counts())\n",
        "\n",
        "all_tasks_assertions['begin'] = all_tasks_assertions['begin'] - all_tasks_assertions['sent_begin']\n",
        "all_tasks_assertions['end'] = all_tasks_assertions['end'] - all_tasks_assertions['sent_begin']\n",
        "\n",
        "print (all_tasks_assertions.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HJJgszFHkQXv",
        "outputId": "c96ec30d-9693-488d-9ef4-7fdff436d8d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1029, 14)\n",
            "(1029, 14)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>task_id</th>\n",
              "      <th>sentence</th>\n",
              "      <th>tkn_start</th>\n",
              "      <th>tkn_end</th>\n",
              "      <th>chunk</th>\n",
              "      <th>entity</th>\n",
              "      <th>assertion_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>46</td>\n",
              "      <td>Sample Type / Medical Specialty:\\nHematology -...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>Hematology</td>\n",
              "      <td>Clinical_Dept</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>46</td>\n",
              "      <td>Sample Type / Medical Specialty:\\nHematology -...</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>Oncology</td>\n",
              "      <td>Clinical_Dept</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>46</td>\n",
              "      <td>Sample Type / Medical Specialty:\\nHematology -...</td>\n",
              "      <td>7</td>\n",
              "      <td>10</td>\n",
              "      <td>Non-Small Cell Lung Cancer</td>\n",
              "      <td>Oncological</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>46</td>\n",
              "      <td>Sample Type / Medical Specialty:\\nHematology -...</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>Description:</td>\n",
              "      <td>Section_Header</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>46</td>\n",
              "      <td>Sample Type / Medical Specialty:\\nHematology -...</td>\n",
              "      <td>15</td>\n",
              "      <td>18</td>\n",
              "      <td>non-small cell lung cancer</td>\n",
              "      <td>Oncological</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>46</td>\n",
              "      <td>Sample Type / Medical Specialty:\\nHematology -...</td>\n",
              "      <td>19</td>\n",
              "      <td>20</td>\n",
              "      <td>stage IV</td>\n",
              "      <td>Modifier</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>46</td>\n",
              "      <td>Sample Type / Medical Specialty:\\nHematology -...</td>\n",
              "      <td>21</td>\n",
              "      <td>22</td>\n",
              "      <td>metastatic disease</td>\n",
              "      <td>Oncological</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>46</td>\n",
              "      <td>At this point, he and his wife ask about wheth...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>he</td>\n",
              "      <td>Gender</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>46</td>\n",
              "      <td>At this point, he and his wife ask about wheth...</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>his</td>\n",
              "      <td>Gender</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>46</td>\n",
              "      <td>At this point, he and his wife ask about wheth...</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>wife</td>\n",
              "      <td>Gender</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>46</td>\n",
              "      <td>(Medical Transcription Sample Report)\\nREASON ...</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>REASON FOR CONSULTATION:</td>\n",
              "      <td>Section_Header</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>46</td>\n",
              "      <td>(Medical Transcription Sample Report)\\nREASON ...</td>\n",
              "      <td>8</td>\n",
              "      <td>11</td>\n",
              "      <td>non-small cell lung cancer</td>\n",
              "      <td>Oncological</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>46</td>\n",
              "      <td>HISTORY OF PRESENT ILLNESS:\\nABCD is a very ni...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>HISTORY OF PRESENT ILLNESS:</td>\n",
              "      <td>Section_Header</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>46</td>\n",
              "      <td>HISTORY OF PRESENT ILLNESS:\\nABCD is a very ni...</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>47-year-old</td>\n",
              "      <td>Age</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>46</td>\n",
              "      <td>HISTORY OF PRESENT ILLNESS:\\nABCD is a very ni...</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>gentleman</td>\n",
              "      <td>Gender</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>46</td>\n",
              "      <td>HISTORY OF PRESENT ILLNESS:\\nABCD is a very ni...</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>new</td>\n",
              "      <td>Modifier</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>46</td>\n",
              "      <td>HISTORY OF PRESENT ILLNESS:\\nABCD is a very ni...</td>\n",
              "      <td>27</td>\n",
              "      <td>28</td>\n",
              "      <td>stage IV</td>\n",
              "      <td>Modifier</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>46</td>\n",
              "      <td>HISTORY OF PRESENT ILLNESS:\\nABCD is a very ni...</td>\n",
              "      <td>29</td>\n",
              "      <td>30</td>\n",
              "      <td>metastatic disease</td>\n",
              "      <td>Oncological</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>46</td>\n",
              "      <td>ABCD and his wife state that his history goes ...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>his</td>\n",
              "      <td>Gender</td>\n",
              "      <td>absent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>46</td>\n",
              "      <td>ABCD and his wife state that his history goes ...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>wife</td>\n",
              "      <td>Gender</td>\n",
              "      <td>absent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>46</td>\n",
              "      <td>ABCD and his wife state that his history goes ...</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>his</td>\n",
              "      <td>Gender</td>\n",
              "      <td>absent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>46</td>\n",
              "      <td>ABCD and his wife state that his history goes ...</td>\n",
              "      <td>12</td>\n",
              "      <td>14</td>\n",
              "      <td>2-2-1/2 weeks ago</td>\n",
              "      <td>RelativeDate</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>46</td>\n",
              "      <td>ABCD and his wife state that his history goes ...</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>he</td>\n",
              "      <td>Gender</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>46</td>\n",
              "      <td>ABCD and his wife state that his history goes ...</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "      <td>left-sided</td>\n",
              "      <td>Direction</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>46</td>\n",
              "      <td>ABCD and his wife state that his history goes ...</td>\n",
              "      <td>20</td>\n",
              "      <td>21</td>\n",
              "      <td>flank pain</td>\n",
              "      <td>Symptom</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>46</td>\n",
              "      <td>Initially, he did not think much of this and t...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>he</td>\n",
              "      <td>Gender</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>46</td>\n",
              "      <td>Initially, he did not think much of this and t...</td>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>did not think much of this and tried to go abo...</td>\n",
              "      <td>Symptom</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>46</td>\n",
              "      <td>Initially, he did not think much of this and t...</td>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "      <td>pain</td>\n",
              "      <td>Symptom</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>46</td>\n",
              "      <td>Initially, he did not think much of this and t...</td>\n",
              "      <td>21</td>\n",
              "      <td>22</td>\n",
              "      <td>gradually worsened</td>\n",
              "      <td>Modifier</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>46</td>\n",
              "      <td>Eventually this prompted him to present to the...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>him</td>\n",
              "      <td>Gender</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>46</td>\n",
              "      <td>Eventually this prompted him to present to the...</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>emergency room</td>\n",
              "      <td>Clinical_Dept</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>46</td>\n",
              "      <td>A CT scan was done there, and he was found to ...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>CT scan</td>\n",
              "      <td>Test</td>\n",
              "      <td>absent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>46</td>\n",
              "      <td>A CT scan was done there, and he was found to ...</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>he</td>\n",
              "      <td>Gender</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>46</td>\n",
              "      <td>A CT scan was done there, and he was found to ...</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>large</td>\n",
              "      <td>Modifier</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>46</td>\n",
              "      <td>A CT scan was done there, and he was found to ...</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>left</td>\n",
              "      <td>Direction</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>46</td>\n",
              "      <td>A CT scan was done there, and he was found to ...</td>\n",
              "      <td>15</td>\n",
              "      <td>16</td>\n",
              "      <td>adrenal mass</td>\n",
              "      <td>Symptom</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>46</td>\n",
              "      <td>At that point, he was transferred to XYZ Hospi...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>he</td>\n",
              "      <td>Gender</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>46</td>\n",
              "      <td>At that point, he was transferred to XYZ Hospi...</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>XYZ Hospital</td>\n",
              "      <td>Clinical_Dept</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>46</td>\n",
              "      <td>On admission on 12/19/08, a CT scan of the che...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>admission</td>\n",
              "      <td>Admission_Discharge</td>\n",
              "      <td>absent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>46</td>\n",
              "      <td>On admission on 12/19/08, a CT scan of the che...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>12/19/08</td>\n",
              "      <td>Date</td>\n",
              "      <td>absent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>46</td>\n",
              "      <td>On admission on 12/19/08, a CT scan of the che...</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>CT scan</td>\n",
              "      <td>Test</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>46</td>\n",
              "      <td>On admission on 12/19/08, a CT scan of the che...</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>chest</td>\n",
              "      <td>External_body_part_or_region</td>\n",
              "      <td>absent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>46</td>\n",
              "      <td>On admission on 12/19/08, a CT scan of the che...</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>abdomen</td>\n",
              "      <td>External_body_part_or_region</td>\n",
              "      <td>absent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>46</td>\n",
              "      <td>On admission on 12/19/08, a CT scan of the che...</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>pelvis</td>\n",
              "      <td>Internal_organ_or_component</td>\n",
              "      <td>absent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>46</td>\n",
              "      <td>The CT scan of the chest showed an abnormal so...</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>CT scan of the chest</td>\n",
              "      <td>Test</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>46</td>\n",
              "      <td>The CT scan of the chest showed an abnormal so...</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>abnormal</td>\n",
              "      <td>Modifier</td>\n",
              "      <td>absent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>46</td>\n",
              "      <td>The CT scan of the chest showed an abnormal so...</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>right</td>\n",
              "      <td>Direction</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>46</td>\n",
              "      <td>The CT scan of the chest showed an abnormal so...</td>\n",
              "      <td>15</td>\n",
              "      <td>16</td>\n",
              "      <td>paratracheal region</td>\n",
              "      <td>External_body_part_or_region</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>46</td>\n",
              "      <td>The CT scan of the chest showed an abnormal so...</td>\n",
              "      <td>20</td>\n",
              "      <td>21</td>\n",
              "      <td>precarinal region</td>\n",
              "      <td>Internal_organ_or_component</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>46</td>\n",
              "      <td>The CT scan of the chest showed an abnormal so...</td>\n",
              "      <td>23</td>\n",
              "      <td>24</td>\n",
              "      <td>subcarinal region</td>\n",
              "      <td>Internal_organ_or_component</td>\n",
              "      <td>present</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    task_id  ... assertion_label\n",
              "0        46  ...         present\n",
              "1        46  ...         present\n",
              "2        46  ...         present\n",
              "3        46  ...         present\n",
              "4        46  ...         present\n",
              "5        46  ...         present\n",
              "6        46  ...         present\n",
              "7        46  ...         present\n",
              "8        46  ...         present\n",
              "9        46  ...         present\n",
              "10       46  ...         present\n",
              "11       46  ...         present\n",
              "12       46  ...         present\n",
              "13       46  ...         present\n",
              "14       46  ...         present\n",
              "15       46  ...         present\n",
              "16       46  ...         present\n",
              "17       46  ...         present\n",
              "18       46  ...          absent\n",
              "19       46  ...          absent\n",
              "20       46  ...          absent\n",
              "21       46  ...         present\n",
              "22       46  ...         present\n",
              "23       46  ...         present\n",
              "24       46  ...         present\n",
              "25       46  ...         present\n",
              "26       46  ...         present\n",
              "27       46  ...         present\n",
              "28       46  ...         present\n",
              "29       46  ...         present\n",
              "30       46  ...         present\n",
              "31       46  ...          absent\n",
              "32       46  ...         present\n",
              "33       46  ...         present\n",
              "34       46  ...         present\n",
              "35       46  ...         present\n",
              "36       46  ...         present\n",
              "37       46  ...         present\n",
              "38       46  ...          absent\n",
              "39       46  ...          absent\n",
              "40       46  ...         present\n",
              "41       46  ...          absent\n",
              "42       46  ...          absent\n",
              "43       46  ...          absent\n",
              "44       46  ...         present\n",
              "45       46  ...          absent\n",
              "46       46  ...         present\n",
              "47       46  ...         present\n",
              "48       46  ...         present\n",
              "49       46  ...         present\n",
              "\n",
              "[50 rows x 7 columns]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from itertools import groupby\n",
        "\n",
        "def split_get_ind(str_):\n",
        "    ret = []\n",
        "    for k, g in groupby(enumerate(str_), lambda x: x[1] != ' '):\n",
        "        if k:\n",
        "            pos, first_item = next(g)\n",
        "            res = first_item + ''.join([x for _, x in g])\n",
        "            ret.append( (pos, pos+len(res)))\n",
        "    return ret\n",
        "\n",
        "tkn_st = []\n",
        "tkn_ed = []\n",
        "for i, row in all_tasks_assertions.iterrows():\n",
        "    ass_tkns = split_get_ind(row['sentence'])\n",
        "    st = -1\n",
        "    ed = -1\n",
        "    for tkn_ind, tkn in enumerate(ass_tkns):\n",
        "        if int(row['begin']) in range(*tkn):\n",
        "            st = tkn_ind\n",
        "        if int(row['end']) in range(*tkn):\n",
        "            ed = tkn_ind\n",
        "    if st < 0 or ed < 0:\n",
        "        tkn_st.append(None)\n",
        "        tkn_ed.append(None)\n",
        "    else:\n",
        "        tkn_st.append(st)\n",
        "        tkn_ed.append(ed)\n",
        "    \n",
        "    #print (st, ed)\n",
        "all_tasks_assertions['tkn_start'] = tkn_st\n",
        "all_tasks_assertions['tkn_end'] = tkn_ed\n",
        "print (all_tasks_assertions.shape)\n",
        "all_tasks_assertions.dropna(inplace=True)\n",
        "all_tasks_assertions.reset_index(inplace=True, drop=True)\n",
        "print (all_tasks_assertions.shape)\n",
        "all_tasks_assertions = all_tasks_assertions[['task_id', 'sentence', 'tkn_start', 'tkn_end', 'chunk', 'entity', 'assertion_label']]\n",
        "all_tasks_assertions.head(50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB2vKrYA-AII"
      },
      "source": [
        "**2.3 Generate Data for Training Relation Extraction Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKjpR32o-oKG"
      },
      "outputs": [],
      "source": [
        "from sparknlp_jsl.training import *\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "json_path = './project_export.json'\n",
        "\n",
        "rdr = AnnotationToolJsonReader(assertion_labels = ['present', 'absent', 'possbile', 'hypothetical', 'conditional', 'associated_with_someone_else'])\n",
        "\n",
        "df_anns = rdr.readDataset(spark, json_path).withColumn(\"json\",F.lit(json_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79wWkQdw-oKQ"
      },
      "outputs": [],
      "source": [
        "rel_pds = df_anns.toPandas()\n",
        "print (rel_pds.shape)\n",
        "print (rel_pds['task_id'].nunique())\n",
        "rel_pds = rel_pds.sort_values('completion_id').drop_duplicates('task_id', keep='last')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPT16YRD-AIJ"
      },
      "outputs": [],
      "source": [
        "rel_pds = rel_pds[rel_pds['relations'].apply(lambda x: len(x)) > 0]\n",
        "\n",
        "all_tasks_relations = []\n",
        "for index, group in rel_pds.groupby('task_id'):\n",
        "    \n",
        "    print (index)\n",
        "    \n",
        "    ann_chunks = pd.DataFrame( [{'chunk_id': i.metadata['chunk_id'], 'chunk': i.result, 'begin': int(i.begin), 'end': i.end, 'entity': i.metadata['entity']} for i in group['tool_chunk'].explode() ] )\n",
        "        \n",
        "    ner_chunks = pd.DataFrame( [{'chunk_num': ii, 'chunk': i.result, 'begin': int(i.begin), 'end': i.end, 'sentence_id': int(i.metadata['sentence'])} for ii, i in enumerate(group['ner_chunk'].explode()) ] )\n",
        "    \n",
        "    with_sent = pd.merge(ann_chunks[['chunk_id', 'begin', 'entity']], ner_chunks, on=['begin'], how='inner')\n",
        "    \n",
        "    sentences_df = pd.DataFrame( [{'sentence_id': int(i.metadata['sentence']), 'sentence': i.result, 'sent_begin': int(i.begin), 'sent_end': int(i.end)} for i in group['sentence'].explode() ] )\n",
        "    \n",
        "    with_sent = pd.merge(with_sent, sentences_df, on=['sentence_id'], how='inner')\n",
        "    \n",
        "    relations_df = pd.DataFrame( [ i.metadata for i in group['relations'].explode() ] )\n",
        "    \n",
        "    relations_df = relations_df.merge(with_sent.rename(columns={'chunk_id': 'chunk_id_1'}),\n",
        "                   on=['chunk_id_1'], how='inner')\n",
        "     \n",
        "    relations_df['relations'] = [i.result for i in group['relations'].explode()]\n",
        "    \n",
        "    relations_df['task_id'] = index\n",
        "    \n",
        "    all_tasks_relations.append(relations_df)\n",
        "    \n",
        "all_tasks_relations = pd.concat(all_tasks_relations, axis=0)\n",
        "\n",
        "all_tasks_relations['entity_begin1'] = all_tasks_relations['entity_begin1'].astype(int) - all_tasks_relations['sent_begin']\n",
        "all_tasks_relations['entity_begin2'] = all_tasks_relations['entity_begin2'].astype(int) - all_tasks_relations['sent_begin']\n",
        "all_tasks_relations['entity_end1'] = all_tasks_relations['entity_end1'].astype(int) - all_tasks_relations['sent_begin']\n",
        "all_tasks_relations['entity_end2'] = all_tasks_relations['entity_end2'].astype(int) - all_tasks_relations['sent_begin']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV6VBTQMVH8P"
      },
      "source": [
        "# 3. Simply Upload data to an Alab Project (without pre-annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "yjXuypwHVH8P",
        "outputId": "0c908709-1a90-44ea-f58e-eb77d07cd03b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://annotationlab.johnsnowlabs.com/api/projects/dummy/import\n",
            "200\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{\"code\":500,\"description\":\"The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.\",\"error\":\"Internal Server Error\"}\\n'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def create_sample_data(text_list):\n",
        "    sample_data_for_upload = []\n",
        "    for index, text in enumerate(text_list):\n",
        "        sample_data_for_upload.append({'title': index, 'text': text})\n",
        "\n",
        "    return sample_data_for_upload\n",
        "\n",
        "sample_data_for_upload = create_sample_data(sample_data['text'].values)\n",
        "\n",
        "project_name = 'demo_100'\n",
        "url = \"https://annotationlab.johnsnowlabs.com/api/projects/{}/import\".format(project_name)\n",
        "print (url)\n",
        "\n",
        "headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"accept\": \"*/*\"\n",
        "    }\n",
        "\n",
        "cookies = get_cookies(username, password)\n",
        "\n",
        "resp = requests.post(url, headers = headers, cookies = cookies, json = sample_data_for_upload)\n",
        "\n",
        "resp.status_code\n",
        "resp.text"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "AL_API_import_export_pre_annotate.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Environment (conda_tensorflow_p36)",
      "language": "python",
      "name": "conda_tensorflow_p36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}